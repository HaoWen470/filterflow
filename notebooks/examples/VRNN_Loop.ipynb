{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "# add to path\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "import attr\n",
    "import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import seaborn\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "\n",
    "tf.config.set_visible_devices([], 'GPU')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sonnet as snt\n",
    "\n",
    "from filterflow.smc import SMC\n",
    "from filterflow.base import State, StateWithMemory, StateSeries, DTYPE_TO_STATE_SERIES\n",
    "\n",
    "from filterflow.observation.base import ObservationModelBase, ObservationSampler\n",
    "from filterflow.observation.linear import LinearObservationSampler\n",
    "from filterflow.transition.random_walk import RandomWalkModel\n",
    "from filterflow.proposal import BootstrapProposalModel\n",
    "from filterflow.transition.base import TransitionModelBase\n",
    "\n",
    "from filterflow.resampling.criterion import NeffCriterion, AlwaysResample, NeverResample, _neff\n",
    "from filterflow.resampling.standard import SystematicResampler, MultinomialResampler\n",
    "from filterflow.resampling.differentiable import RegularisedTransform, CorrectedRegularizedTransform, PartiallyCorrectedRegularizedTransform\n",
    "\n",
    "from filterflow.resampling.base import NoResampling\n",
    "\n",
    "from filterflow.state_space_model import StateSpaceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(0)\n",
    "filter_seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "from scipy.sparse import coo_matrix\n",
    "data_dir = \"../../data/piano_data\"\n",
    "\n",
    "def sparse_pianoroll_to_dense(pianoroll, min_note, num_notes):\n",
    "    \"\"\"Converts a sparse pianoroll to a dense numpy array.\n",
    "    Given a sparse pianoroll, converts it to a dense numpy array of shape\n",
    "    [num_timesteps, num_notes] where entry i,j is 1.0 if note j is active on\n",
    "    timestep i and 0.0 otherwise.\n",
    "    Args:\n",
    "    pianoroll: A sparse pianoroll object, a list of tuples where the i'th tuple\n",
    "      contains the indices of the notes active at timestep i.\n",
    "    min_note: The minimum note in the pianoroll, subtracted from all notes so\n",
    "      that the minimum note becomes 0.\n",
    "    num_notes: The number of possible different note indices, determines the\n",
    "      second dimension of the resulting dense array.\n",
    "    Returns:\n",
    "    dense_pianoroll: A [num_timesteps, num_notes] numpy array of floats.\n",
    "    num_timesteps: A python int, the number of timesteps in the pianoroll.\n",
    "    \"\"\"\n",
    "    num_timesteps = len(pianoroll)\n",
    "    inds = []\n",
    "    for time, chord in enumerate(pianoroll):\n",
    "        # Re-index the notes to start from min_note.\n",
    "        inds.extend((time, note-min_note) for note in chord)\n",
    "        shape = [num_timesteps, num_notes]\n",
    "    values = [1.] * len(inds)\n",
    "    sparse_pianoroll = coo_matrix(\n",
    "      (values, ([x[0] for x in inds], [x[1] for x in inds])),\n",
    "      shape=shape)\n",
    "    return sparse_pianoroll.toarray(), num_timesteps\n",
    "\n",
    "def create_pianoroll_dataset(path,\n",
    "                             split,\n",
    "                             batch_size,\n",
    "                             num_parallel_calls=4,\n",
    "                             shuffle=False,\n",
    "                             repeat=False,\n",
    "                             min_note=21,\n",
    "                             max_note=108):\n",
    "    \"\"\"Creates a pianoroll dataset.\n",
    "    Args:\n",
    "    path: The path of a pickle file containing the dataset to load.\n",
    "    split: The split to use, can be train, test, or valid.\n",
    "    batch_size: The batch size. If repeat is False then it is not guaranteed\n",
    "      that the true batch size will match for all batches since batch_size\n",
    "      may not necessarily evenly divide the number of elements.\n",
    "    num_parallel_calls: The number of threads to use for parallel processing of\n",
    "      the data.\n",
    "    shuffle: If true, shuffles the order of the dataset.\n",
    "    repeat: If true, repeats the dataset endlessly.\n",
    "    min_note: The minimum note number of the dataset. For all pianoroll datasets\n",
    "      the minimum note is number 21, and changing this affects the dimension of\n",
    "      the data. This is useful mostly for testing.\n",
    "    max_note: The maximum note number of the dataset. For all pianoroll datasets\n",
    "      the maximum note is number 108, and changing this affects the dimension of\n",
    "      the data. This is useful mostly for testing.\n",
    "    Returns:\n",
    "    inputs: A batch of input sequences represented as a dense Tensor of shape\n",
    "      [time, batch_size, data_dimension]. The sequences in inputs are the\n",
    "      sequences in targets shifted one timestep into the future, padded with\n",
    "      zeros. This tensor is mean-centered, with the mean taken from the pickle\n",
    "      file key 'train_mean'.\n",
    "    targets: A batch of target sequences represented as a dense Tensor of\n",
    "      shape [time, batch_size, data_dimension].\n",
    "    lens: An int Tensor of shape [batch_size] representing the lengths of each\n",
    "      sequence in the batch.\n",
    "    mean: A float Tensor of shape [data_dimension] containing the mean loaded\n",
    "      from the pickle file.\n",
    "    \"\"\"\n",
    "    # Load the data from disk.\n",
    "    num_notes = max_note - min_note + 1\n",
    "    with tf.io.gfile.GFile(path, \"rb\") as f:\n",
    "        raw_data = pickle.load(f)\n",
    "    pianorolls = raw_data[split]\n",
    "    mean = raw_data[\"train_mean\"]\n",
    "    num_examples = len(pianorolls)\n",
    "\n",
    "    def pianoroll_generator():\n",
    "        for sparse_pianoroll in pianorolls:\n",
    "            yield sparse_pianoroll_to_dense(sparse_pianoroll, min_note, num_notes)\n",
    "\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "      pianoroll_generator,\n",
    "      output_types=(tf.float64, tf.int64),\n",
    "      output_shapes=([None, num_notes], []))\n",
    "\n",
    "    if repeat: \n",
    "        dataset = dataset.repeat()\n",
    "    if shuffle: \n",
    "        dataset = dataset.shuffle(num_examples)\n",
    "\n",
    "    # Batch sequences togther, padding them to a common length in time.\n",
    "    dataset = dataset.padded_batch(batch_size,\n",
    "                                 padded_shapes=([None, num_notes], []))\n",
    "\n",
    "    def process_pianoroll_batch(data, lengths):\n",
    "        \"\"\"Create mean-centered and time-major next-step prediction Tensors.\"\"\"\n",
    "        data = tf.cast(tf.transpose(data, perm=[1, 0, 2]), float)\n",
    "        lengths = tf.cast(lengths, tf.int32)\n",
    "        targets = data\n",
    "        # Mean center the inputs.\n",
    "        inputs = data - tf.constant(mean, dtype=tf.float32,\n",
    "                                    shape=[1, 1, mean.shape[0]])\n",
    "        # Shift the inputs one step forward in time. Also remove the last timestep\n",
    "        # so that targets and inputs are the same length.\n",
    "        inputs = tf.pad(inputs, [[1, 0], [0, 0], [0, 0]], mode=\"CONSTANT\")[:-1]\n",
    "        # Mask out unused timesteps.\n",
    "        inputs *= tf.expand_dims(tf.transpose(\n",
    "            tf.sequence_mask(lengths, dtype=inputs.dtype)), 2)\n",
    "        return inputs, targets, lengths\n",
    "\n",
    "    dataset = dataset.map(process_pianoroll_batch,\n",
    "                        num_parallel_calls=num_parallel_calls)\n",
    "    dataset = dataset.prefetch(num_examples)\n",
    "\n",
    "    itr = tf.compat.v1.data.make_one_shot_iterator(dataset)\n",
    "    inputs, targets, lengths = itr.get_next()\n",
    "    return inputs, targets, lengths, tf.constant(mean, dtype=tf.float32)\n",
    "\n",
    "path = os.path.join(data_dir, 'jsb.pkl')\n",
    "inputs_tensor, targets_tensor, lens, mean = create_pianoroll_dataset(path, split='train', batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=float32, numpy=array([0., 0., 1., 2.], dtype=float32)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = tf.constant([0.,1.,2.,3.])\n",
    "@tf.function()\n",
    "def pad(test):\n",
    "    return tf.pad(test, [[1, 0]], mode=\"CONSTANT\")[:-1]\n",
    "\n",
    "pad(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "T = targets_tensor.shape.as_list()[0]\n",
    "observation_size = targets_tensor.shape.as_list()[-1]\n",
    "\n",
    "\n",
    "latent_size = 10\n",
    "fcnet_hidden_sizes = [latent_size]\n",
    "encoded_data_size = latent_size\n",
    "rnn_hidden_size = latent_size//2\n",
    "\n",
    "latent_encoder_layers = [32]\n",
    "latent_encoded_size = 32\n",
    "\n",
    "latent_encoder = snt.nets.MLP(\n",
    "            output_sizes=latent_encoder_layers + [latent_encoded_size],\n",
    "            name=\"latent_encoder\")\n",
    "\n",
    "data_encoder_layers = [32]\n",
    "encoded_data_size = 32\n",
    "data_encoder = snt.nets.MLP(\n",
    "            output_sizes=data_encoder_layers + [encoded_data_size],\n",
    "            name=\"data_encoder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store observations\n",
    "batch_size = 1\n",
    "n_particles = 25\n",
    "dimension = latent_size\n",
    "\n",
    "inputs_tensor = tf.expand_dims(inputs_tensor, 1)\n",
    "targets_tensor = tf.expand_dims(targets_tensor, 1)\n",
    "\n",
    "obs_data = tf.data.Dataset.from_tensor_slices(targets_tensor)\n",
    "inputs_data = tf.data.Dataset.from_tensor_slices(inputs_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNNormalDistribution(tf.Module):\n",
    "    \"\"\"A Normal distribution with mean and var parametrised by NN\"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 size, \n",
    "                 hidden_layer_sizes, \n",
    "                 sigma_min=0.0,\n",
    "                 raw_sigma_bias=0.25, \n",
    "                 hidden_activation_fn=tf.nn.relu,\n",
    "                 name=\"conditional_normal_distribution\"):\n",
    "        \n",
    "        super(NNNormalDistribution, self).__init__(name=name)\n",
    "        \n",
    "        self.sigma_min = sigma_min\n",
    "        self.raw_sigma_bias = raw_sigma_bias\n",
    "        self.size = size\n",
    "        self.fcnet = snt.nets.MLP(\n",
    "            output_sizes=hidden_layer_sizes + [2*size],\n",
    "            activation=hidden_activation_fn,\n",
    "            activate_final=False,\n",
    "            name=name + \"_fcnet\")\n",
    "\n",
    "    def get_params(self, tensor_list, **unused_kwargs):\n",
    "        \"\"\"Computes the parameters of a normal distribution based on the inputs.\"\"\"\n",
    "        inputs = tf.concat(tensor_list, axis=-1)\n",
    "        outs = self.fcnet(inputs)\n",
    "        mu, sigma = tf.split(outs, 2, axis=-1)\n",
    "        sigma = tf.maximum(tf.nn.softplus(sigma + self.raw_sigma_bias), self.sigma_min)\n",
    "        return mu, sigma\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        \"\"\"Creates a normal distribution conditioned on the inputs.\"\"\"\n",
    "        mu, sigma = self.get_params(args, **kwargs)\n",
    "        return tfp.distributions.Normal(loc=mu, scale=sigma)\n",
    "    \n",
    "class NNBernoulliDistribution(tf.Module):\n",
    "    \"\"\"A Normal distribution with mean and var parametrised by NN\"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 size, \n",
    "                 hidden_layer_sizes, \n",
    "                 hidden_activation_fn=tf.nn.relu,\n",
    "                 name=\"conditional_bernoulli_distribution\"):\n",
    "        super(NNBernoulliDistribution, self).__init__(name=name)\n",
    "        \n",
    "        self.size = size\n",
    "        self.fcnet = snt.nets.MLP(\n",
    "            output_sizes=hidden_layer_sizes + [size],\n",
    "            activation=hidden_activation_fn,\n",
    "            activate_final=False,\n",
    "            name=name + \"_fcnet\")\n",
    "\n",
    "    def get_logits(self, tensor_list, **unused_kwargs):\n",
    "        \"\"\"Computes the parameters of a normal distribution based on the inputs.\"\"\"\n",
    "        inputs = tf.concat(tensor_list, axis=-1)\n",
    "        return self.fcnet(inputs)\n",
    "        \n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        \"\"\"Creates a normal distribution conditioned on the inputs.\"\"\"\n",
    "        logits = self.get_logits(args, **kwargs)\n",
    "        return tfp.distributions.Bernoulli(logits=logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VRNNTransitionModel(TransitionModelBase):\n",
    "    def __init__(self, \n",
    "                 rnn_hidden_size, \n",
    "                 data_encoder, \n",
    "                 latent_encoder,\n",
    "                 name='NNTransitionModel'):\n",
    "        \n",
    "        super(VRNNTransitionModel, self).__init__(name=name)\n",
    "        \n",
    "        # mlp parametrised gaussian\n",
    "        self.transition = NNNormalDistribution(size=latent_size, \n",
    "                                               hidden_layer_sizes=[latent_size])\n",
    "        # encoder for inputs\n",
    "        self.latent_encoder = latent_encoder\n",
    "        \n",
    "        # lstm cell\n",
    "        self.rnn_hidden_size = rnn_hidden_size\n",
    "        self.rnn = tf.keras.layers.LSTMCell(rnn_hidden_size)\n",
    "        \n",
    "    def run_rnn(self, state: State, inputs: tf.Tensor):\n",
    "        \n",
    "        tiled_inputs = tf.tile(inputs, [state.batch_size, state.n_particles, 1])\n",
    "        # process latent state\n",
    "        latent_state = state.particles\n",
    "        \n",
    "        # encode and reshape latent state\n",
    "        latent_encoded = self.latent_encoder(latent_state)\n",
    "        \n",
    "        B, N, D = latent_encoded.shape\n",
    "        # process rnn_state\n",
    "        rnn_state = tf.reshape(state.rnn_state, [B,  N, self.rnn_hidden_size*2])\n",
    "\n",
    "        rnn_state = tf.split(rnn_state, 2, axis=-1)\n",
    "        \n",
    "\n",
    "        # run rnn\n",
    "        rnn_inputs = tf.concat([tiled_inputs, latent_encoded], axis=-1)\n",
    "        rnn_inputs_reshaped = tf.reshape(rnn_inputs, (B*N, -1))\n",
    "        rnn_state_reshaped = [tf.reshape(elem, (B*N, -1)) for elem in rnn_state]\n",
    "        rnn_out, rnn_state = self.rnn(rnn_inputs_reshaped, rnn_state_reshaped)\n",
    "\n",
    "\n",
    "        rnn_state = tf.concat(rnn_state, axis=-1)\n",
    "        rnn_state = tf.reshape(rnn_state, [state.batch_size, state.n_particles, self.rnn_hidden_size*2])\n",
    "        rnn_out = tf.reshape(rnn_out, [state.batch_size, state.n_particles, self.rnn_hidden_size])\n",
    "        return rnn_out, rnn_state, latent_encoded\n",
    "    \n",
    "    def latent_dist(self, state, rnn_out):\n",
    "        dist = self.transition(rnn_out)\n",
    "        return dist\n",
    "\n",
    "    def loglikelihood(self, prior_state: State, proposed_state: State, inputs: tf.Tensor):\n",
    "        rnn_out, rnn_state, latent_encoded = self.run_rnn(prior_state, inputs)\n",
    "        dist = self.transition(rnn_out)\n",
    "        new_latent = proposed_state.particles\n",
    "        return tf.reduce_sum(dist.log_prob(new_latent), axis=-1)\n",
    "\n",
    "    def sample(self, state: State, inputs: tf.Tensor, seed=None):\n",
    "        \n",
    "        rnn_out, rnn_state, latent_encoded = self.run_rnn(state, inputs)\n",
    "        dist = self.latent_dist(state, rnn_out)\n",
    "        latent_state = dist.sample(seed=seed)\n",
    "        \n",
    "        return VRNNState(particles=latent_state, \n",
    "                          log_weights = state.log_weights,\n",
    "                          weights=state.weights, \n",
    "                          log_likelihoods=state.log_likelihoods,\n",
    "                          rnn_state = rnn_state,\n",
    "                          rnn_out = rnn_out,\n",
    "                          latent_encoded =  latent_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VRNNProposalModel(VRNNTransitionModel):\n",
    "    def __init__(self, \n",
    "                 rnn_hidden_size, \n",
    "                 data_encoder, \n",
    "                 latent_encoder,\n",
    "                 name='VRNNProposalModel'):\n",
    "        \n",
    "        super(VRNNProposalModel, self).__init__(rnn_hidden_size, \n",
    "                 data_encoder, \n",
    "                 latent_encoder,name)\n",
    "\n",
    "    def loglikelihood(self, proposed_state: State, state: State, inputs: tf.Tensor, observation: tf.Tensor):\n",
    "        rnn_out, rnn_state, latent_encoded = self.run_rnn(state, inputs)\n",
    "        dist = self.latent_dist(state, rnn_out)\n",
    "        new_latent = proposed_state.particles\n",
    "        return tf.reduce_sum(dist.log_prob(new_latent), axis=-1)\n",
    "    \n",
    "    def propose(self, state: State, inputs: tf.Tensor, observation: tf.Tensor, seed=None):\n",
    "        return self.sample(state, inputs, seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VRNNBernoulliObservationModel(ObservationSampler):\n",
    "    \n",
    "    def __init__(self, latent_encoder, observation_size, name='VRNNObservationModel'):\n",
    "        super(VRNNBernoulliObservationModel, self).__init__(name=name)\n",
    "        # mlp parametrised gaussian\n",
    "        self.emission = NNBernoulliDistribution(size=observation_size, \n",
    "                                                hidden_layer_sizes=[observation_size])\n",
    "        \n",
    "    \n",
    "    def observation_dist(self, state: State):\n",
    "        latent_state = state.particles\n",
    "        latent_encoded = state.latent_encoded\n",
    "        rnn_out = state.rnn_out\n",
    "        dist = self.emission(latent_encoded, rnn_out)\n",
    "        return dist\n",
    "    \n",
    "    def loglikelihood(self, state: State, observation: tf.Tensor):\n",
    "        dist = self.observation_dist(state)\n",
    "        return tf.reduce_sum(dist.log_prob(observation), axis=-1)\n",
    "        \n",
    "\n",
    "    def sample(self, state: State):\n",
    "        dist = self.observation_dist(state)\n",
    "        return dist.sample()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@attr.s\n",
    "class VRNNState(State):\n",
    "    ADDITIONAL_STATE_VARIABLES = ('rnn_state',) # rnn_out and encoded no need to be resampled\n",
    "    rnn_state = attr.ib(default=None)\n",
    "    rnn_out = attr.ib(default=None)\n",
    "    latent_encoded = attr.ib(default=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_model = VRNNTransitionModel(rnn_hidden_size, data_encoder, latent_encoder)\n",
    "observation_model = VRNNBernoulliObservationModel(latent_encoder, observation_size)\n",
    "proposal_model = VRNNProposalModel(rnn_hidden_size, data_encoder, latent_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial state\n",
    "normal_dist = tfp.distributions.Normal(0., 1.)\n",
    "initial_latent_state = tf.zeros([batch_size, n_particles, dimension])\n",
    "initial_latent_state = tf.cast(initial_latent_state, dtype=float)\n",
    "latent_encoded = transition_model.latent_encoder(initial_latent_state)\n",
    "\n",
    "# initial rnn_state\n",
    "initial_rnn_state = [normal_dist.sample([batch_size,n_particles,rnn_hidden_size])]*2\n",
    "initial_rnn_state = tf.concat(initial_rnn_state, axis=-1)\n",
    "\n",
    "# rnn_out\n",
    "initial_rnn_out = tf.zeros([batch_size, n_particles, rnn_hidden_size])\n",
    "\n",
    "initial_weights = tf.ones((batch_size, n_particles), dtype=float) / tf.cast(n_particles, float)\n",
    "log_likelihoods = tf.zeros(batch_size, dtype=float)\n",
    "initial_state = VRNNState(particles=initial_latent_state, \n",
    "                          log_weights = tf.math.log(initial_weights),\n",
    "                          weights=initial_weights, \n",
    "                          log_likelihoods=log_likelihoods,\n",
    "                          rnn_state = initial_rnn_state,\n",
    "                          rnn_out = initial_rnn_out,\n",
    "                          latent_encoded = latent_encoded)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# snt networks initiated on first call\n",
    "t_samp = transition_model.sample(initial_state, inputs_tensor[0])\n",
    "obs_samp = observation_model.sample(initial_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latent_encoder/linear_0/b:0\n",
      "latent_encoder/linear_0/w:0\n",
      "latent_encoder/linear_1/b:0\n",
      "latent_encoder/linear_1/w:0\n",
      "lstm_cell/kernel:0\n",
      "lstm_cell/recurrent_kernel:0\n",
      "lstm_cell/bias:0\n",
      "conditional_normal_distribution_fcnet/linear_0/b:0\n",
      "conditional_normal_distribution_fcnet/linear_0/w:0\n",
      "conditional_normal_distribution_fcnet/linear_1/b:0\n",
      "conditional_normal_distribution_fcnet/linear_1/w:0\n"
     ]
    }
   ],
   "source": [
    "for var in transition_model.variables:\n",
    "    print(var.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conditional_bernoulli_distribution_fcnet/linear_0/b:0\n",
      "conditional_bernoulli_distribution_fcnet/linear_0/w:0\n",
      "conditional_bernoulli_distribution_fcnet/linear_1/b:0\n",
      "conditional_bernoulli_distribution_fcnet/linear_1/w:0\n"
     ]
    }
   ],
   "source": [
    "for var in observation_model.variables:\n",
    "    print(var.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Particle Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable_variables = transition_model.variables + observation_model.variables\n",
    "init_values = [v.value() for v in trainable_variables]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = n_particles\n",
    "B = batch_size\n",
    "\n",
    "# initial state\n",
    "normal_dist = tfp.distributions.Normal(0., 1.)\n",
    "initial_latent_state = tf.zeros([B, N, dimension])\n",
    "initial_latent_state = tf.cast(initial_latent_state, dtype=float)\n",
    "latent_encoded = transition_model.latent_encoder(initial_latent_state)\n",
    "\n",
    "# initial rnn_state\n",
    "initial_rnn_state = [normal_dist.sample([B, N, rnn_hidden_size], seed=filter_seed)]*2\n",
    "initial_rnn_state = tf.concat(initial_rnn_state, axis=-1)\n",
    "\n",
    "# rnn_out\n",
    "initial_rnn_out = tf.zeros([B, N, rnn_hidden_size])\n",
    "\n",
    "initial_weights = tf.ones((B, N), dtype=float) / tf.cast(N, float)\n",
    "log_likelihoods = tf.zeros(B, dtype=float)\n",
    "\n",
    "init_state = VRNNState(particles=initial_latent_state, \n",
    "                          log_weights = tf.math.log(initial_weights),\n",
    "                          weights=initial_weights, \n",
    "                          log_likelihoods=log_likelihoods,\n",
    "                          rnn_state=initial_rnn_state,\n",
    "                          rnn_out=initial_rnn_out,\n",
    "                          latent_encoded=latent_encoded)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "resampling_criterion = NeffCriterion(tf.constant(0.9), tf.constant(True), verbose=False)\n",
    "#resampling_criterion = AlwaysResample()\n",
    "resampling_method = MultinomialResampler()\n",
    "\n",
    "epsilon = tf.constant(0.5)\n",
    "scaling = tf.constant(0.9)\n",
    "\n",
    "regularized = RegularisedTransform(epsilon, scaling=scaling, max_iter=1000, convergence_threshold=1e-3)\n",
    "\n",
    "    \n",
    "multinomial_smc = SMC(observation_model, transition_model, proposal_model, resampling_criterion, resampling_method)\n",
    "regularized_smc = SMC(observation_model, transition_model, proposal_model, resampling_criterion, regularized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_optimizer(initial_learning_rate = 0.01, decay_steps=100, decay_rate=0.75, staircase=True):\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate,\n",
    "        decay_steps=decay_steps,\n",
    "        decay_rate=decay_rate,\n",
    "        staircase=staircase)\n",
    "\n",
    "    optimizer = tf.optimizers.Adam(learning_rate=lr_schedule)\n",
    "    return optimizer\n",
    "\n",
    "def run_smc(smc, optimizer, n_iter):\n",
    "    @tf.function\n",
    "    def smc_routine(smc, state, use_correction_term=False, seed=filter_seed):\n",
    "        final_state = smc(state, obs_data, n_observations=T, inputs_series=inputs_data, return_final=True, seed=filter_seed)\n",
    "        res = tf.reduce_mean(final_state.log_likelihoods)\n",
    "        if use_correction_term:\n",
    "            return res, tf.reduce_mean(final_state.resampling_correction)\n",
    "        return res, tf.constant(0.)\n",
    "\n",
    "\n",
    "    @tf.function\n",
    "    def run_one_step(smc, use_correction_term, init_state):\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(trainable_variables)\n",
    "            real_ll, correction = smc_routine(smc, init_state, use_correction_term)\n",
    "            loss = -(real_ll + correction)\n",
    "        grads_loss = tape.gradient(loss, trainable_variables)\n",
    "        return real_ll, grads_loss\n",
    "\n",
    "    @tf.function\n",
    "    def train_one_step(smc, use_correction_term):\n",
    "        real_ll, grads_loss = run_one_step(smc, use_correction_term, init_state)\n",
    "        capped_gvs = [tf.clip_by_value(grad, -500., 500.) for grad in grads_loss]\n",
    "        optimizer.apply_gradients(zip(capped_gvs, trainable_variables))\n",
    "        return -real_ll, capped_gvs\n",
    "\n",
    "    @tf.function\n",
    "    def train_niter(smc, num_steps=100, use_correction_term=False, reset=True):\n",
    "        if reset:\n",
    "            reset_operations = [v.assign(init) for v, init in zip(trainable_variables, init_values)]\n",
    "        else:\n",
    "            reset_operations = []\n",
    "        loss_tensor_array = tf.TensorArray(dtype=tf.float32, size=num_steps, dynamic_size=False, element_shape=[])\n",
    "        grad_tensor_array = tf.TensorArray(dtype=tf.float32, size=num_steps, dynamic_size=False, element_shape=[])\n",
    "        time_tensor_array = tf.TensorArray(dtype=tf.float64, size=num_steps, dynamic_size=False, element_shape=[])\n",
    "        with tf.control_dependencies(reset_operations):\n",
    "            toc = tf.constant(0., dtype=tf.float64)\n",
    "            tic = tf.timestamp()\n",
    "            for step in tf.range(1, num_steps+1):\n",
    "\n",
    "                tic_loss = tf.timestamp()\n",
    "                with tf.control_dependencies([tic_loss]):\n",
    "                    loss, grads = train_one_step(smc, use_correction_term)\n",
    "                with tf.control_dependencies([loss]):\n",
    "                    toc_loss = tf.timestamp()            \n",
    "\n",
    "\n",
    "                toc += toc_loss - tic_loss\n",
    "\n",
    "                max_grad = tf.reduce_max([tf.reduce_max(tf.abs(grad)) for grad in grads])\n",
    "\n",
    "                tf.print('Step', step, '/', num_steps, ': ms per step= ', 1000. * toc / tf.cast(step, tf.float64), ': total compute time (s)= ', toc, 'Real Time elapsed (s): ', tf.timestamp()-tic, ', loss = ', loss, ', max abs grads = ', max_grad, end='\\r')\n",
    "\n",
    "                loss_tensor_array = loss_tensor_array.write(step-1, -loss)\n",
    "                grad_tensor_array = grad_tensor_array.write(step-1, max_grad)\n",
    "                time_tensor_array = time_tensor_array.write(step-1, toc)\n",
    "        return loss_tensor_array.stack(), grad_tensor_array.stack(), time_tensor_array.stack()\n",
    "        \n",
    "    return train_niter(smc, tf.constant(n_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /data/hylia/thornton/venvs/filter_venv/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Step 100 / 100 : ms per step=  183.26578140258789 : total compute time (s)=  18.326578140258789 Real Time elapsed (s):  18.40349006652832 , loss =  924.354675 , max abs grads =  500.9755863\r"
     ]
    }
   ],
   "source": [
    "n_iter = 100\n",
    "\n",
    "multinomial_ll_n_epochs, _, multinomial_time = run_smc(multinomial_smc, make_optimizer(), n_iter)\n",
    "multinomial_ll_n_epochs_numpy = multinomial_ll_n_epochs.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f73602cf990>]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(multinomial_ll_n_epochs_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 100 / 100 : ms per step=  1595.1579427719116 : total compute time (s)=  159.51579427719116 Real Time elapsed (s):  159.59555816650391 , loss =  791.593079 , max abs grads =  78.7562943\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-bb8d63adee57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mreg_ll_n_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_smc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mregularized_smc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmake_optimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mreg_ll_n_epochs_numpy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreg_ll_n_epochs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-932f9040e6e6>\u001b[0m in \u001b[0;36mrun_smc\u001b[0;34m(smc, optimizer, n_iter)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss_tensor_array\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensor_array\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_tensor_array\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrain_niter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/data/hylia/thornton/venvs/filter_venv/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/hylia/thornton/venvs/filter_venv/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    648\u001b[0m               *args, **kwds)\n\u001b[1;32m    649\u001b[0m       \u001b[0;31m# If we did not create any variables the trace we have is good enough.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 650\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_concrete_stateful_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcanon_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfn_with_cond\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minner_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0minner_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/hylia/thornton/venvs/filter_venv/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/hylia/thornton/venvs/filter_venv/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/hylia/thornton/venvs/filter_venv/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/data/hylia/thornton/venvs/filter_venv/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "reg_ll_n_epochs, _, reg_time = run_smc(regularized_smc, make_optimizer(), n_iter)\n",
    "reg_ll_n_epochs_numpy = reg_ll_n_epochs.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(multinomial_ll_n_epochs_numpy, color='blue')\n",
    "ax.plot(reg_ll_n_epochs_numpy, color='green')\n",
    "fig.savefig(os.path.join('./charts/', 'vrnn_loss_per_epoch.png'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warmup = 100\n",
    "end = 500\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(multinomial_ll_n_epochs_numpy[warmup:end], color='blue')\n",
    "ax.plot(reg_ll_n_epochs_numpy[warmup:end], color='green')\n",
    "fig.savefig(os.path.join('./charts/', 'vrnn_loss_per_epoch.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_identifier(initial_lr, decay, steps, method):\n",
    "    return \"method_{3}__lr0_{0}__decay_{1}__steps_{2}\".format(initial_lr, decay, steps, method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "def pickle_obj(obj, file_path):\n",
    "    with open(file_path, 'wb') as handle:\n",
    "        pickle.dump(obj, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "def unpickle_obj(file_path):\n",
    "    with open(file_path, 'rb') as handle:\n",
    "        obj = pickle.load(handle)\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Mult\n",
      "Step 500 / 500 : ms per step=  167.22277784347534 : total compute time (s)=  83.611388921737671 Real Time elapsed (s):  84.005030155181885 , loss =  871.77771 , max abs grads =  30.50878147\n",
      " Reg\n",
      "Step 23 / 500 : ms per step=  1668.46167522928 : total compute time (s)=  38.374618530273438 Real Time elapsed (s):  38.39284610748291 , loss =  1352.59058 , max abs grads =  57.3509712862\r"
     ]
    }
   ],
   "source": [
    "n_iter = 500\n",
    "warmup = 100\n",
    "end = n_iter\n",
    "out_dir = './charts/'\n",
    "for initial_lr in [0.01, 0.05, 0.1,0.25]:\n",
    "    for decay in [0.5, 0.75, 0.9]:\n",
    "        for steps in [50, 100, 150]:\n",
    "            opt_fn = lambda : make_optimizer(initial_learning_rate = initial_lr, \n",
    "                                             decay_steps=steps, decay_rate=decay, staircase=True)\n",
    "            \n",
    "            print(\"\\n Mult\")\n",
    "            multinomial_ll_n_epochs, _, multinomial_time = run_smc(multinomial_smc, opt_fn(), n_iter)\n",
    "            multinomial_ll_n_epochs_numpy = multinomial_ll_n_epochs.numpy()\n",
    "            mult_key = fn_identifier(initial_lr, decay, steps, 'mult')\n",
    "            pickle_obj(multinomial_ll_n_epochs_numpy, os.path.join(out_dir, \"vrnn_loss_{0}.pkl\".format(mult_key)))\n",
    "            \n",
    "            fig, ax = plt.subplots(figsize=(10, 5))\n",
    "            ax.plot(multinomial_ll_n_epochs_numpy[warmup:end], color='blue')\n",
    "            fig.savefig(os.path.join(out_dir, 'vrnn_loss_{0}.png'.format(mult_key)))\n",
    "            \n",
    "            \n",
    "            print(\"\\n Reg\")\n",
    "            reg_ll_n_epochs, _, reg_time = run_smc(regularized_smc, opt_fn(), n_iter)\n",
    "            reg_ll_n_epochs_numpy = reg_ll_n_epochs.numpy()\n",
    "            reg_key = fn_identifier(initial_lr, decay, steps, 'reg')\n",
    "            pickle_obj(reg_ll_n_epochs_numpy, os.path.join(out_dir, \"vrnn_loss_{0}.pkl\".format(reg_key)))\n",
    "            \n",
    "            fig, ax = plt.subplots(figsize=(10, 5))\n",
    "            ax.plot(reg_ll_n_epochs_numpy[warmup:end], color='green')\n",
    "            fig.savefig(os.path.join(out_dir, 'vrnn_loss_{0}.png'.format(reg_key)))\n",
    "            \n",
    "            both_key = fn_identifier(initial_lr, decay, steps, 'both')\n",
    "            fig, ax = plt.subplots(figsize=(10, 5))\n",
    "            ax.plot(multinomial_ll_n_epochs_numpy[warmup:end], color='blue')\n",
    "            ax.plot(reg_ll_n_epochs_numpy[warmup:end], color='green')\n",
    "            fig.savefig(os.path.join(out_dir, 'vrnn_loss_{0}.png'.format(both_key)))\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "filter_venv",
   "language": "python",
   "name": "filter_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
