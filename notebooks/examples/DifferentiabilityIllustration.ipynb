{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "import attr\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.set_visible_devices([], 'GPU') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from filterflow.base import State\n",
    "from filterflow.resampling.standard import SystematicResampler, StratifiedResampler, MultinomialResampler\n",
    "from filterflow.resampling.base import NoResampling\n",
    "from filterflow.resampling.differentiable import RegularisedTransform, CorrectedRegularizedTransform\n",
    "from filterflow.resampling.differentiable.optimized import OptimizedPointCloud\n",
    "from filterflow.resampling.differentiable.optimizer.sgd import SGD\n",
    "from filterflow.resampling.differentiable.ricatti.solver import PetkovSolver\n",
    "from filterflow.resampling.differentiable.loss.sliced_wasserstein.swd import SlicedWassersteinDistance\n",
    "from filterflow.resampling.differentiable.loss.regularized import SinkhornLoss\n",
    "from filterflow.resampling.differentiable.loss.sliced_wasserstein.utils import sqeuclidean, norm_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook aims to demonstrate the differentiability issue encountered at resampling time. To do this we will compare functionals of the point cloud whilst changing a parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(222)\n",
    "\n",
    "B = 1\n",
    "N = 25\n",
    "D = 1\n",
    "\n",
    "x = tf.random.normal([B, N, D], 0., 1.)\n",
    "y = tf.zeros(D)\n",
    "\n",
    "log_weights = tf.zeros([B, N]) - math.log(N)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(state, observation, resampler):\n",
    "    rv = tfp.distributions.MultivariateNormalDiag(tf.zeros(D), tf.ones(D))\n",
    "    flags = tf.constant([True])\n",
    "    log_prob = rv.log_prob(observation-state.particles)\n",
    "    log_weights = log_prob - tf.reduce_logsumexp(log_prob, 1, keepdims=True)\n",
    "    state = attr.evolve(state, log_weights=log_weights, weights=tf.math.exp(log_weights))\n",
    "    state = resampler.apply(state, flags)\n",
    "    log_prob = rv.log_prob(observation-state.particles) + state.log_weights\n",
    "    return tf.reduce_logsumexp(log_prob)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_data = 150\n",
    "linspace = np.linspace(-0.5, 0.5, n_data).astype(np.float32)\n",
    "linspace_dataset = tf.data.Dataset.from_tensor_slices(linspace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def get_data_no_reset_seed(linspace, resampler, x, y):\n",
    "    res = tf.TensorArray(dtype=tf.float32, dynamic_size=False, size=n_data)\n",
    "    grads = tf.TensorArray(dtype=tf.float32, dynamic_size=False, size=n_data)\n",
    "    i = tf.constant(0, dtype=tf.int64)\n",
    "    s = tf.constant(0., dtype=tf.float64)\n",
    "    total_tic = tf.timestamp()\n",
    "    for i, z_val in linspace_dataset.enumerate():\n",
    "        tic = tf.timestamp()\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(z_val)\n",
    "            z = z_val + tf.zeros(D)\n",
    "            state = State(x + z, log_weights, tf.math.exp(log_weights), tf.constant([0.]), None)\n",
    "            ll = log_likelihood(state, y, resampler)\n",
    "        ll_grad = tape.gradient(ll, z_val)\n",
    "        j = tf.cast(i, tf.int32)\n",
    "        res = res.write(j, ll)\n",
    "\n",
    "        grads = grads.write(j, ll_grad)\n",
    "        toc = tf.timestamp()\n",
    "        s = s + (toc - tic)\n",
    "        tf.print('Step', i+1, '/', n_data, ', seconds/iter =', s / tf.cast(i+1, tf.float64), end='\\r')\n",
    "    total_toc = tf.timestamp()\n",
    "\n",
    "    tf.print('Step', i+1, '/', n_data, ', total time (seconds) =', total_toc - total_tic)\n",
    "    return res.stack(), grads.stack()\n",
    "\n",
    "# do not decorate this. seed is being set\n",
    "def get_data(linspace, resampler, x, y, seed):\n",
    "    res = []\n",
    "    grads = []\n",
    "    for z_val in tqdm.tqdm(linspace):\n",
    "        z_ = tf.constant(z_val)\n",
    "        tf.random.set_seed(seed)\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(z_)\n",
    "            z = z_ + tf.zeros(D)\n",
    "            state = State(x + z, log_weights, tf.math.exp(log_weights), tf.constant([0.]), None)\n",
    "            ll = log_likelihood(state, y, resampler)\n",
    "        ll_grad = tape.gradient(ll, z)\n",
    "        res.append(ll.numpy().sum())\n",
    "        grads.append(ll_grad.numpy().sum())\n",
    "        \n",
    "    return res, grads\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'stop_gradient'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-f290bc321594>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0msystematic\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSystematicResampler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0msystematic_unbiased\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSystematicResampler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstop_gradient\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mmultinomial\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMultinomialResampler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mstratified\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStratifiedResampler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mno_resampling\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNoResampling\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'stop_gradient'"
     ]
    }
   ],
   "source": [
    "systematic = SystematicResampler()\n",
    "multinomial = MultinomialResampler()\n",
    "stratified = StratifiedResampler()\n",
    "no_resampling = NoResampling()\n",
    "\n",
    "epsilon = tf.constant(0.5)\n",
    "scaling = tf.constant(0.75)\n",
    "convergence_threshold = tf.constant(1e-4)\n",
    "max_iter = tf.constant(500)\n",
    "\n",
    "regularized = RegularisedTransform(epsilon, scaling, max_iter, convergence_threshold)\n",
    "\n",
    "\n",
    "solver = PetkovSolver(n_iter=tf.constant(50))\n",
    "corrected_no_grad = CorrectedRegularizedTransform(epsilon, scaling, max_iter, convergence_threshold, ricatti_solver=solver, propagate_correction_gradient=False)\n",
    "corrected = CorrectedRegularizedTransform(epsilon, scaling, max_iter, convergence_threshold, ricatti_solver=solver, propagate_correction_gradient=True)\n",
    "\n",
    "sinkhorn_loss = SinkhornLoss(epsilon, symmetric=True, scaling=scaling, max_iter=tf.constant(100), convergence_threshold=convergence_threshold)\n",
    "sinkhorn_optimizer = SGD(sinkhorn_loss, 1., 50, 0.9)\n",
    "sinkhorn_optimized_cloud = OptimizedPointCloud(sinkhorn_optimizer, regularized)\n",
    "\n",
    "sliced_loss = SlicedWassersteinDistance(15, sqeuclidean)\n",
    "sliced_optimizer = SGD(sliced_loss, 1., 50, 0.9)\n",
    "sliced_optimized_cloud = OptimizedPointCloud(sliced_optimizer, regularized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no_resampling_data, no_resampling_grad = get_data_no_reset_seed(linspace, no_resampling, x, y)\n",
    "systematic_data, systematic_grad = get_data(linspace, systematic, x, y, 42)\n",
    "# multinomial_data, multinomial_grad = get_data(linspace, multinomial, x, y, 51)\n",
    "# stratified_data, stratified_grad = get_data(linspace, stratified, x, y, 44)\n",
    "\n",
    "# regularized_data, regularized_grad = get_data_no_reset_seed(linspace, regularized, x, y)\n",
    "# corrected_no_grad_data, corrected_no_grad_grad = get_data_no_reset_seed(linspace, corrected_no_grad, x, y)\n",
    "# corrected_data, corrected_grad = get_data_no_reset_seed(linspace, corrected, x, y)\n",
    "# sliced_optimized_data, sliced_optimized_grad = get_data_no_reset_seed(linspace, sliced_optimized_cloud, x, y)\n",
    "# sinkhorn_optimized_data, sinkhorn_optimized_grad = get_data_no_reset_seed(linspace, sinkhorn_optimized_cloud, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=2, figsize=(15, 5), sharex=True, sharey=True)\n",
    "axes[0].plot(linspace, no_resampling_data, label='no resampling', linestyle='--', color='k')\n",
    "axes[1].plot(linspace, no_resampling_data, label='no resampling', linestyle='--', color='k')\n",
    "axes[0].step(linspace, systematic_data, label='systematic', alpha=0.75)\n",
    "axes[0].step(linspace, multinomial_data, label='multinomial', alpha=0.75)\n",
    "axes[0].step(linspace, stratified_data, label='stratified', alpha=0.75)\n",
    "axes[1].plot(linspace, regularized_data, label='regularized')\n",
    "axes[1].plot(linspace, corrected_data, label='corrected')\n",
    "axes[1].plot(linspace, corrected_no_grad_data, label='corrected_no_grad')\n",
    "axes[1].plot(linspace, sliced_optimized_data, label='sliced_optimized')\n",
    "axes[1].plot(linspace, sinkhorn_optimized_data, label='sinkhorn_optimized')\n",
    "_ = axes[0].legend(), axes[1].legend()\n",
    "fig.savefig(os.path.join('./charts/', 'differentiability_illustration_likelihood.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=2, figsize=(15, 5), sharex=True, sharey=False)\n",
    "axes[0].step(linspace, no_resampling_grad, label='no resampling', linestyle='--', color='k')\n",
    "axes[1].step(linspace, no_resampling_grad, label='no resampling', linestyle='--', color='k')\n",
    "axes[0].step(linspace, systematic_grad, label='systematic', alpha=0.75)\n",
    "axes[0].step(linspace, multinomial_grad, label='multinomial', alpha=0.75)\n",
    "axes[0].step(linspace, stratified_grad, label='stratified', alpha=0.75)\n",
    "axes[1].plot(linspace, regularized_grad, label='regularized')\n",
    "axes[1].plot(linspace, corrected_grad, label='corrected')\n",
    "axes[1].plot(linspace, corrected_no_grad_grad, label='corrected_no_grad')\n",
    "# axes[1].plot(linspace, sliced_optimized_data, label='sliced_optimized')\n",
    "axes[1].plot(linspace, sinkhorn_optimized_grad, label='sinkhorn_optimized')\n",
    "_ = axes[0].legend(), axes[1].legend()\n",
    "fig.savefig(os.path.join('./charts/', 'differentiability_illustration_gradient.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
