{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn the globally optimal proposal for a linear gaussian state space model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reproduce the experiment in Naesseth and al. Variational SMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "# add to path\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "import attr\n",
    "import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pykalman\n",
    "import seaborn\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "ed = tfp.edward2\n",
    "\n",
    "seaborn.set()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.set_visible_devices([], 'GPU') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from filterflow.smc import SMC\n",
    "from filterflow.base import State, StateSeries\n",
    "\n",
    "from filterflow.observation.linear import LinearObservationModel\n",
    "\n",
    "from filterflow.transition.random_walk import RandomWalkModel\n",
    "from filterflow.proposal import BootstrapProposalModel\n",
    "from filterflow.proposal.base import ProposalModelBase\n",
    "\n",
    "from filterflow.resampling.criterion import NeffCriterion, AlwaysResample, NeverResample\n",
    "from filterflow.resampling.standard import SystematicResampler, MultinomialResampler\n",
    "from filterflow.resampling.differentiable import RegularisedTransform, CorrectedRegularizedTransform\n",
    "from filterflow.resampling.differentiable.ricatti.solver import PetkovSolver, NaiveSolver\n",
    "\n",
    "from filterflow.resampling.base import NoResampling\n",
    "\n",
    "from filterflow.state_space_model import StateSpaceModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-383.28591316387576"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(0)\n",
    "rndstate = np.random.RandomState(0)\n",
    "\n",
    "T = 25\n",
    "\n",
    "d_x = 10\n",
    "d_y = 10\n",
    "\n",
    "alpha = 0.42\n",
    "gamma = 0.1\n",
    "\n",
    "B = 1\n",
    "N = 4\n",
    "\n",
    "transition_matrix = np.array([[alpha**(abs(i-j) + 1) for j in range(d_x)] for i in range(d_x)], dtype=np.float32)\n",
    "transition_covariance = np.eye(d_x, dtype=np.float32)\n",
    "\n",
    "\n",
    "observation_covariance = gamma * np.eye(d_y, dtype=np.float32)\n",
    "observation_matrix = np.zeros([d_y, d_x], dtype=np.float32)\n",
    "observation_matrix[:d_y, :d_y] = np.eye(d_y, dtype=np.float32)\n",
    "# observation_matrix = np.ones([d_y, d_x], dtype=np.float32)\n",
    "# observation_matrix = np.random.normal(0., 1., [d_y, d_x]).astype(np.float32)\n",
    "\n",
    "\n",
    "initial_state_mean = np.zeros(d_x, dtype=np.float32)\n",
    "initial_state_covariance = np.eye(d_x, dtype=np.float32)\n",
    "\n",
    "kf = pykalman.KalmanFilter(transition_matrix, observation_matrix, transition_covariance, observation_covariance, initial_state_mean=initial_state_mean, initial_state_covariance=initial_state_covariance)\n",
    "observations = kf.sample(T, random_state=rndstate)[1].data.astype(np.float32)\n",
    "\n",
    "true_log_likelihood = kf.loglikelihood(observations)\n",
    "true_log_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "observations_dataset = tf.data.Dataset.from_tensor_slices(observations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the filterflow filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_noise = tfd.MultivariateNormalDiag(tf.zeros(d_x, dtype=tf.float32), tf.linalg.diag_part(transition_covariance) ** 0.5) # mind it MultivariateNormalDiag does diag * diag...\n",
    "transition_model = RandomWalkModel(tf.convert_to_tensor(transition_matrix), transition_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_error = tfd.MultivariateNormalDiag(tf.zeros(d_y, dtype=tf.float32), tf.linalg.diag_part(observation_covariance) ** 0.5) # mind it MultivariateNormalDiag does diag * diag...\n",
    "observation_model = LinearObservationModel(tf.convert_to_tensor(observation_matrix), observation_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearnableProposalModel(ProposalModelBase):\n",
    "    def __init__(self, transition_matrix, name='LearnableProposalModel'):\n",
    "        super(LearnableProposalModel, self).__init__(name=name)\n",
    "        self._transition_matrix = transition_matrix\n",
    "        self._standard_noise = tfd.MultivariateNormalDiag(tf.zeros(transition_matrix.shape[0]), tf.ones(transition_matrix.shape[0]))\n",
    "    \n",
    "    def propose(self, state: State, inputs, _observation: tf.Tensor):\n",
    "        \"\"\"See base class\"\"\"\n",
    "        mu_t, beta_t, sigma_t = inputs\n",
    "        \n",
    "        transition_matrix = tf.linalg.matmul(tf.linalg.diag(beta_t), self._transition_matrix)\n",
    "        \n",
    "        pushed_particles = tf.reshape(mu_t, [1, 1, -1]) + tf.linalg.matvec(transition_matrix, state.particles)\n",
    "        \n",
    "        scale = tfp.bijectors.ScaleMatvecDiag(sigma_t)\n",
    "        scaled_rv = tfd.TransformedDistribution(self._standard_noise, bijector=scale)        \n",
    "        proposed_particles = pushed_particles + scaled_rv.sample([state.batch_size, state.n_particles])\n",
    "        return attr.evolve(state, particles=proposed_particles)\n",
    "\n",
    "    def loglikelihood(self, proposed_state: State, state: State, inputs: tf.Tensor, observation: tf.Tensor):\n",
    "        \"\"\"Interface method for particle proposal\n",
    "        :param proposed_state: State\n",
    "            proposed state\n",
    "        :param state: State\n",
    "            previous particle filter state\n",
    "        :param inputs: tf.Tensor\n",
    "            Control variables (time elapsed, some environment variables, etc)\n",
    "        :param observation: tf.Tensor\n",
    "            Look ahead observation for adapted particle proposal\n",
    "        :return: proposed State\n",
    "        :rtype: tf.Tensor\n",
    "        \"\"\"            \n",
    "        mu_t, beta_t, sigma_t = inputs\n",
    "        transition_matrix = tf.linalg.matmul(tf.linalg.diag(beta_t), self._transition_matrix)\n",
    "        pushed_particles = tf.reshape(mu_t, [1, 1, -1]) + tf.linalg.matvec(transition_matrix, state.particles)\n",
    "        \n",
    "        scale = tfp.bijectors.ScaleMatvecDiag(sigma_t)\n",
    "        scaled_rv = tfd.TransformedDistribution(self._standard_noise, bijector=scale)\n",
    "        \n",
    "        diff = (pushed_particles - proposed_state.particles)\n",
    "        return scaled_rv.log_prob(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "proposal_model = LearnableProposalModel(transition_matrix)\n",
    "bootstrap_proposal = BootstrapProposalModel(transition_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "resampling_criterion = NeffCriterion(tf.constant(0.5), tf.constant(True))\n",
    "resampling_method = MultinomialResampler()\n",
    "\n",
    "epsilon = tf.constant(0.5)\n",
    "scaling = tf.constant(0.75)\n",
    "\n",
    "regularized = RegularisedTransform(epsilon, scaling=scaling, max_iter=1000, convergence_threshold=1e-1)\n",
    "\n",
    "solver = PetkovSolver(tf.constant(30))\n",
    "corrected = CorrectedRegularizedTransform(epsilon, scaling=scaling, max_iter=1000, convergence_threshold=1e-1, ricatti_solver=solver, propagate_correction_gradient=False)\n",
    "\n",
    "    \n",
    "systematic_smc = SMC(observation_model, transition_model, proposal_model, resampling_criterion, resampling_method)\n",
    "regularized_smc = SMC(observation_model, transition_model, proposal_model, resampling_criterion, regularized)\n",
    "corrected_smc = SMC(observation_model, transition_model, proposal_model, resampling_criterion, corrected)\n",
    "bootstrap_smc = SMC(observation_model, transition_model, bootstrap_proposal, resampling_criterion, resampling_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_particles = tfd.MultivariateNormalDiag(initial_state_mean, tf.linalg.diag_part(initial_state_covariance)**0.5).sample([B, N])\n",
    "uniform_weights = tf.ones([B, N])\n",
    "uniform_weights = uniform_weights / tf.reduce_sum(uniform_weights, 1, keepdims=True)\n",
    "\n",
    "uniform_log_weights = tf.math.log(uniform_weights)\n",
    "log_likelihoods = tf.zeros([B])\n",
    "init_state = State(initial_particles, uniform_log_weights, uniform_weights, log_likelihoods, None, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the learnable variables that will be used as inputs to the proposal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 0.5\n",
    "\n",
    "mu_ts_init = scale * tf.random.normal([T, d_x])\n",
    "beta_ts_init = 1. + scale * tf.random.normal([T, d_x])\n",
    "log_sigma_ts_init = scale * tf.random.normal([T, d_x])\n",
    "\n",
    "mu_ts_init = tf.zeros([T, d_x])\n",
    "beta_ts_init = tf.ones([T, d_x])\n",
    "log_sigma_ts_init = tf.zeros([T, d_x])\n",
    "\n",
    "\n",
    "mu_ts = tf.Variable(mu_ts_init, trainable=True)\n",
    "beta_ts = tf.Variable(beta_ts_init, trainable=True)\n",
    "log_sigma_ts = tf.Variable(log_sigma_ts_init, trainable=True)\n",
    "\n",
    "# beta_ts = []\n",
    "# sigma_ts = []\n",
    "# for t in range(T):\n",
    "#     mu_ts.append(tf.Variable(0., trainable=True))\n",
    "#     beta_ts.append(tf.Variable(1., trainable=True))\n",
    "#     sigma_ts.append(tf.Variable(gamma, trainable=True))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable_variables = [mu_ts, beta_ts, log_sigma_ts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def smc_routine(smc, state, use_correction_term=False):\n",
    "    iterator = iter(observations_dataset)\n",
    "    for t in tf.range(T):\n",
    "        mu_t = mu_ts[t]\n",
    "        beta_t = beta_ts[t]\n",
    "        sigma_t = tf.math.exp(log_sigma_ts[t])\n",
    "        obs = iterator.get_next()\n",
    "        state = smc.update(state, obs, [mu_t, beta_t, sigma_t])\n",
    "        t = t + 1\n",
    "    res = tf.reduce_mean(state.log_likelihoods)\n",
    "    if use_correction_term:\n",
    "        return res, tf.reduce_mean(state.resampling_correction)\n",
    "    return res, tf.constant(0.)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.optimizers.Adam(learning_rate=1e-2, epsilon=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def run_one_step(smc, use_correction_term):\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(trainable_variables)\n",
    "        real_ll, correction = smc_routine(smc, init_state, use_correction_term)\n",
    "        loss = -(real_ll + correction)\n",
    "    grads_loss = tape.gradient(loss, trainable_variables)\n",
    "    return real_ll, grads_loss\n",
    "\n",
    "@tf.function\n",
    "def train_one_step(smc, use_correction_term):\n",
    "    real_ll, grads_loss = run_one_step(smc, use_correction_term)\n",
    "    optimizer.apply_gradients(zip(grads_loss, trainable_variables))\n",
    "    return -real_ll, grads_loss\n",
    "\n",
    "@tf.function\n",
    "def train_niter(smc, num_steps=100, use_correction_term=False, reset=True):\n",
    "    if reset:\n",
    "        reset_operations = [mu_ts.assign(mu_ts_init), beta_ts.assign(beta_ts_init), log_sigma_ts.assign(log_sigma_ts_init)]\n",
    "    else:\n",
    "        reset_operations = []\n",
    "    loss_tensor_array = tf.TensorArray(dtype=tf.float32, size=num_steps, dynamic_size=False, element_shape=[])\n",
    "    grad_tensor_array = tf.TensorArray(dtype=tf.float32, size=num_steps, dynamic_size=False, element_shape=[])\n",
    "    time_tensor_array = tf.TensorArray(dtype=tf.float64, size=num_steps, dynamic_size=False, element_shape=[])\n",
    "    with tf.control_dependencies(reset_operations):\n",
    "        tic = tf.timestamp()\n",
    "        for step in tf.range(1, num_steps+1):\n",
    "            loss, grads = train_one_step(smc, use_correction_term)\n",
    "            toc = tf.timestamp()\n",
    "            max_grad = tf.reduce_max(tf.abs(grads))\n",
    "            if step % (num_steps // 100) == 0:\n",
    "                tf.print('Step', step, '/', num_steps, ': ms per step= ', 1000. * (toc - tic) / tf.cast(step, tf.float64), ': total time (s)= ', (toc - tic), ', loss = ', loss, ', max abs grads = ', max_grad, end='\\r')\n",
    "            loss_tensor_array = loss_tensor_array.write(step-1, loss)\n",
    "            grad_tensor_array = grad_tensor_array.write(step-1, max_grad)\n",
    "            time_tensor_array = time_tensor_array.write(step-1, toc-tic)\n",
    "    return loss_tensor_array.stack(), grad_tensor_array.stack(), time_tensor_array.stack()\n",
    "\n",
    "@tf.function\n",
    "def train_total_time(smc, total_time, use_correction_term=False, reset=True):\n",
    "    if reset:\n",
    "        reset_operations = [mu_ts.assign(mu_ts_init), beta_ts.assign(beta_ts_init), log_sigma_ts.assign(log_sigma_ts_init)]\n",
    "    else:\n",
    "        reset_operations = []\n",
    "    loss_tensor_array = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True, element_shape=[])\n",
    "    grad_tensor_array = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True, element_shape=[])\n",
    "    time_tensor_array = tf.TensorArray(dtype=tf.float64, size=0, dynamic_size=True, element_shape=[])\n",
    "    with tf.control_dependencies(reset_operations):\n",
    "        tic = tf.timestamp()\n",
    "        toc = tic\n",
    "        step = tf.constant(1)\n",
    "        while toc - tic < total_time:\n",
    "            loss, grads = train_one_step(smc, use_correction_term)\n",
    "            max_grad = tf.reduce_max(tf.abs(grads))\n",
    "            step = step + 1\n",
    "            loss_tensor_array = loss_tensor_array.write(step-1, loss)\n",
    "            grad_tensor_array = grad_tensor_array.write(step-1, max_grad)\n",
    "            time_tensor_array = time_tensor_array.write(step-1, toc-tic)\n",
    "            toc = tf.timestamp()\n",
    "\n",
    "            tf.print('Time elapsed (s): ', toc-tic, ', n_steps: ', step, ': ms per step= ', 1000. * (toc - tic) / tf.cast(step, tf.float64), end='\\r')\n",
    "    return loss_tensor_array.stack(), grad_tensor_array.stack(), time_tensor_array.stack()\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def run_several(smc, n_times, use_correction_term=False):\n",
    "    loss_array = tf.TensorArray(dtype=tf.float32, size=n_times, dynamic_size=False, element_shape=[])\n",
    "    grad_array = tf.TensorArray(dtype=tf.float32, size=n_times, dynamic_size=False)\n",
    "    for i in tf.range(n_times):\n",
    "        real_ll, grads_loss = run_one_step(smc, use_correction_term)\n",
    "        loss_array = loss_array.write(i, real_ll)        \n",
    "        grad_array = grad_array.write(i, grads_loss)\n",
    "        tf.print('Step: ', i+1, '/', n_times, end='\\r')\n",
    "    return loss_array.stack(), grad_array.stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\linalg\\linear_operator_diag.py:166: calling LinearOperator.__init__ (from tensorflow.python.ops.linalg.linear_operator) with graph_parents is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Do not pass `graph_parents`.  They will  no longer be used.\n",
      "Step 1000 / 1000 : ms per step=  27.361740112304688 : total time (s)=  27.361740112304688 , loss =  389.112457 , max abs grads =  21.6651497\r"
     ]
    }
   ],
   "source": [
    "sys_ll_n_epochs, _, sys_time = train_niter(systematic_smc, tf.constant(n_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:  50 / 50\r"
     ]
    }
   ],
   "source": [
    "sys_ll_per_seed, sys_grad_per_seed = run_several(systematic_smc, tf.constant(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed (s):  33.2975378036499 , n_steps:  1162 : ms per step=  28.6553681614887279881\r"
     ]
    }
   ],
   "source": [
    "sys_ll_total_time, _, _ = train_total_time(systematic_smc, tf.constant(60, dtype=tf.float64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_ll_n_epochs, _, reg_time = train_niter(regularized_smc, tf.constant(n_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_ll_per_seed, reg_grad_per_seed = run_several(regularized_smc, tf.constant(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_ll_total_time, _, _ = train_total_time(regularized_smc, tf.constant(60, dtype=tf.float64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(sys_ll_n_epochs.numpy(), color='blue')\n",
    "ax.plot(reg_ll_n_epochs.numpy(), color='green')\n",
    "ax.hlines(-true_log_likelihood, 0, n_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(np.linspace(0, 60, len(sys_ll_total_time.numpy())), sys_ll_total_time.numpy(), color='blue')\n",
    "ax.plot(np.linspace(0, 60, len(reg_ll_total_time.numpy())), reg_ll_total_time.numpy(), color='green')\n",
    "ax.hlines(-true_log_likelihood, 0, 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_ll_per_seed.numpy().std(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_ll_per_seed.numpy().std(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_grad_df = pd.DataFrame(reg_grad_per_seed.numpy().std(0).reshape(-1, 3), columns = ['$\\mu_t$', r'$\\beta_t$', '$\\ln(\\sigma_t)$'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_grad_df = pd.DataFrame(sys_grad_per_seed.numpy().std(0).reshape(-1, 3), columns = ['$\\mu_t$', r'$\\beta_t$', '$\\ln(\\sigma_t)$'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_grad_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_grad_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
