{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "# add to path\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "import attr\n",
    "import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import seaborn\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "\n",
    "tf.config.set_visible_devices([], 'GPU')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sonnet as snt\n",
    "\n",
    "from filterflow.smc import SMC\n",
    "from filterflow.base import State, StateWithMemory, StateSeries, DTYPE_TO_STATE_SERIES\n",
    "\n",
    "from filterflow.observation.base import ObservationModelBase, ObservationSampler\n",
    "from filterflow.observation.linear import LinearObservationSampler\n",
    "from filterflow.transition.random_walk import RandomWalkModel\n",
    "from filterflow.proposal import BootstrapProposalModel\n",
    "from filterflow.transition.base import TransitionModelBase\n",
    "\n",
    "from filterflow.resampling.criterion import NeffCriterion, AlwaysResample, NeverResample, _neff\n",
    "from filterflow.resampling.standard import SystematicResampler, MultinomialResampler\n",
    "from filterflow.resampling.differentiable import RegularisedTransform, CorrectedRegularizedTransform, PartiallyCorrectedRegularizedTransform\n",
    "\n",
    "from filterflow.resampling.base import NoResampling\n",
    "\n",
    "from filterflow.state_space_model import StateSpaceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(0)\n",
    "filter_seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "from scipy.sparse import coo_matrix\n",
    "data_dir = \"../../data/piano_data\"\n",
    "\n",
    "def sparse_pianoroll_to_dense(pianoroll, min_note, num_notes):\n",
    "    \"\"\"Converts a sparse pianoroll to a dense numpy array.\n",
    "    Given a sparse pianoroll, converts it to a dense numpy array of shape\n",
    "    [num_timesteps, num_notes] where entry i,j is 1.0 if note j is active on\n",
    "    timestep i and 0.0 otherwise.\n",
    "    Args:\n",
    "    pianoroll: A sparse pianoroll object, a list of tuples where the i'th tuple\n",
    "      contains the indices of the notes active at timestep i.\n",
    "    min_note: The minimum note in the pianoroll, subtracted from all notes so\n",
    "      that the minimum note becomes 0.\n",
    "    num_notes: The number of possible different note indices, determines the\n",
    "      second dimension of the resulting dense array.\n",
    "    Returns:\n",
    "    dense_pianoroll: A [num_timesteps, num_notes] numpy array of floats.\n",
    "    num_timesteps: A python int, the number of timesteps in the pianoroll.\n",
    "    \"\"\"\n",
    "    num_timesteps = len(pianoroll)\n",
    "    inds = []\n",
    "    for time, chord in enumerate(pianoroll):\n",
    "        # Re-index the notes to start from min_note.\n",
    "        inds.extend((time, note-min_note) for note in chord)\n",
    "        shape = [num_timesteps, num_notes]\n",
    "    values = [1.] * len(inds)\n",
    "    sparse_pianoroll = coo_matrix(\n",
    "      (values, ([x[0] for x in inds], [x[1] for x in inds])),\n",
    "      shape=shape)\n",
    "    return sparse_pianoroll.toarray(), num_timesteps\n",
    "\n",
    "def create_pianoroll_dataset(path,\n",
    "                             split,\n",
    "                             batch_size,\n",
    "                             num_parallel_calls=4,\n",
    "                             shuffle=False,\n",
    "                             repeat=False,\n",
    "                             min_note=21,\n",
    "                             max_note=108):\n",
    "    \"\"\"Creates a pianoroll dataset.\n",
    "    Args:\n",
    "    path: The path of a pickle file containing the dataset to load.\n",
    "    split: The split to use, can be train, test, or valid.\n",
    "    batch_size: The batch size. If repeat is False then it is not guaranteed\n",
    "      that the true batch size will match for all batches since batch_size\n",
    "      may not necessarily evenly divide the number of elements.\n",
    "    num_parallel_calls: The number of threads to use for parallel processing of\n",
    "      the data.\n",
    "    shuffle: If true, shuffles the order of the dataset.\n",
    "    repeat: If true, repeats the dataset endlessly.\n",
    "    min_note: The minimum note number of the dataset. For all pianoroll datasets\n",
    "      the minimum note is number 21, and changing this affects the dimension of\n",
    "      the data. This is useful mostly for testing.\n",
    "    max_note: The maximum note number of the dataset. For all pianoroll datasets\n",
    "      the maximum note is number 108, and changing this affects the dimension of\n",
    "      the data. This is useful mostly for testing.\n",
    "    Returns:\n",
    "    inputs: A batch of input sequences represented as a dense Tensor of shape\n",
    "      [time, batch_size, data_dimension]. The sequences in inputs are the\n",
    "      sequences in targets shifted one timestep into the future, padded with\n",
    "      zeros. This tensor is mean-centered, with the mean taken from the pickle\n",
    "      file key 'train_mean'.\n",
    "    targets: A batch of target sequences represented as a dense Tensor of\n",
    "      shape [time, batch_size, data_dimension].\n",
    "    lens: An int Tensor of shape [batch_size] representing the lengths of each\n",
    "      sequence in the batch.\n",
    "    mean: A float Tensor of shape [data_dimension] containing the mean loaded\n",
    "      from the pickle file.\n",
    "    \"\"\"\n",
    "    # Load the data from disk.\n",
    "    num_notes = max_note - min_note + 1\n",
    "    with tf.io.gfile.GFile(path, \"rb\") as f:\n",
    "        raw_data = pickle.load(f)\n",
    "    pianorolls = raw_data[split]\n",
    "    mean = raw_data[\"train_mean\"]\n",
    "    num_examples = len(pianorolls)\n",
    "\n",
    "    def pianoroll_generator():\n",
    "        for sparse_pianoroll in pianorolls:\n",
    "            yield sparse_pianoroll_to_dense(sparse_pianoroll, min_note, num_notes)\n",
    "\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "      pianoroll_generator,\n",
    "      output_types=(tf.float64, tf.int64),\n",
    "      output_shapes=([None, num_notes], []))\n",
    "\n",
    "    if repeat: \n",
    "        dataset = dataset.repeat()\n",
    "    if shuffle: \n",
    "        dataset = dataset.shuffle(num_examples)\n",
    "\n",
    "    # Batch sequences togther, padding them to a common length in time.\n",
    "    dataset = dataset.padded_batch(batch_size,\n",
    "                                 padded_shapes=([None, num_notes], []))\n",
    "\n",
    "    def process_pianoroll_batch(data, lengths):\n",
    "        \"\"\"Create mean-centered and time-major next-step prediction Tensors.\"\"\"\n",
    "        data = tf.cast(tf.transpose(data, perm=[1, 0, 2]), float)\n",
    "        lengths = tf.cast(lengths, tf.int32)\n",
    "        targets = data\n",
    "        # Mean center the inputs.\n",
    "        inputs = data - tf.constant(mean, dtype=tf.float32,\n",
    "                                    shape=[1, 1, mean.shape[0]])\n",
    "        # Shift the inputs one step forward in time. Also remove the last timestep\n",
    "        # so that targets and inputs are the same length.\n",
    "        inputs = tf.pad(inputs, [[1, 0], [0, 0], [0, 0]], mode=\"CONSTANT\")[:-1]\n",
    "        # Mask out unused timesteps.\n",
    "        inputs *= tf.expand_dims(tf.transpose(\n",
    "            tf.sequence_mask(lengths, dtype=inputs.dtype)), 2)\n",
    "        return inputs, targets, lengths\n",
    "\n",
    "    dataset = dataset.map(process_pianoroll_batch,\n",
    "                        num_parallel_calls=num_parallel_calls)\n",
    "    dataset = dataset.prefetch(num_examples)\n",
    "\n",
    "    itr = tf.compat.v1.data.make_one_shot_iterator(dataset)\n",
    "    inputs, targets, lengths = itr.get_next()\n",
    "    return inputs, targets, lengths, tf.constant(mean, dtype=tf.float32)\n",
    "\n",
    "path = os.path.join(data_dir, 'jsb.pkl')\n",
    "inputs_tensor, targets_tensor, lens, mean = create_pianoroll_dataset(path, split='train', batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=float32, numpy=array([0., 0., 1., 2.], dtype=float32)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = tf.constant([0.,1.,2.,3.])\n",
    "@tf.function()\n",
    "def pad(test):\n",
    "    return tf.pad(test, [[1, 0]], mode=\"CONSTANT\")[:-1]\n",
    "\n",
    "pad(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "T = targets_tensor.shape.as_list()[0]\n",
    "observation_size = targets_tensor.shape.as_list()[-1]\n",
    "\n",
    "\n",
    "latent_size = 10\n",
    "fcnet_hidden_sizes = [latent_size]\n",
    "encoded_data_size = latent_size\n",
    "rnn_hidden_size = latent_size//2\n",
    "\n",
    "latent_encoder_layers = [32]\n",
    "latent_encoded_size = 32\n",
    "\n",
    "latent_encoder = snt.nets.MLP(\n",
    "            output_sizes=latent_encoder_layers + [latent_encoded_size],\n",
    "            name=\"latent_encoder\")\n",
    "\n",
    "data_encoder_layers = [32]\n",
    "encoded_data_size = 32\n",
    "data_encoder = snt.nets.MLP(\n",
    "            output_sizes=data_encoder_layers + [encoded_data_size],\n",
    "            name=\"data_encoder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store observations\n",
    "batch_size = 1\n",
    "n_particles = 25\n",
    "dimension = latent_size\n",
    "\n",
    "inputs_tensor = tf.expand_dims(inputs_tensor, 1)\n",
    "targets_tensor = tf.expand_dims(targets_tensor, 1)\n",
    "\n",
    "obs_data = tf.data.Dataset.from_tensor_slices(targets_tensor)\n",
    "inputs_data = tf.data.Dataset.from_tensor_slices(inputs_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNNormalDistribution(tf.Module):\n",
    "    \"\"\"A Normal distribution with mean and var parametrised by NN\"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 size, \n",
    "                 hidden_layer_sizes, \n",
    "                 sigma_min=0.0,\n",
    "                 raw_sigma_bias=0.25, \n",
    "                 hidden_activation_fn=tf.nn.relu,\n",
    "                 name=\"conditional_normal_distribution\"):\n",
    "        \n",
    "        super(NNNormalDistribution, self).__init__(name=name)\n",
    "        \n",
    "        self.sigma_min = sigma_min\n",
    "        self.raw_sigma_bias = raw_sigma_bias\n",
    "        self.size = size\n",
    "        self.fcnet = snt.nets.MLP(\n",
    "            output_sizes=hidden_layer_sizes + [2*size],\n",
    "            activation=hidden_activation_fn,\n",
    "            activate_final=False,\n",
    "            name=name + \"_fcnet\")\n",
    "\n",
    "    def get_params(self, tensor_list, **unused_kwargs):\n",
    "        \"\"\"Computes the parameters of a normal distribution based on the inputs.\"\"\"\n",
    "        inputs = tf.concat(tensor_list, axis=-1)\n",
    "        outs = self.fcnet(inputs)\n",
    "        mu, sigma = tf.split(outs, 2, axis=-1)\n",
    "        sigma = tf.maximum(tf.nn.softplus(sigma + self.raw_sigma_bias), self.sigma_min)\n",
    "        return mu, sigma\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        \"\"\"Creates a normal distribution conditioned on the inputs.\"\"\"\n",
    "        mu, sigma = self.get_params(args, **kwargs)\n",
    "        return tfp.distributions.Normal(loc=mu, scale=sigma)\n",
    "    \n",
    "class NNBernoulliDistribution(tf.Module):\n",
    "    \"\"\"A Normal distribution with mean and var parametrised by NN\"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 size, \n",
    "                 hidden_layer_sizes, \n",
    "                 hidden_activation_fn=tf.nn.relu,\n",
    "                 name=\"conditional_bernoulli_distribution\"):\n",
    "        super(NNBernoulliDistribution, self).__init__(name=name)\n",
    "        \n",
    "        self.size = size\n",
    "        self.fcnet = snt.nets.MLP(\n",
    "            output_sizes=hidden_layer_sizes + [size],\n",
    "            activation=hidden_activation_fn,\n",
    "            activate_final=False,\n",
    "            name=name + \"_fcnet\")\n",
    "\n",
    "    def get_logits(self, tensor_list, **unused_kwargs):\n",
    "        \"\"\"Computes the parameters of a normal distribution based on the inputs.\"\"\"\n",
    "        inputs = tf.concat(tensor_list, axis=-1)\n",
    "        return self.fcnet(inputs)\n",
    "        \n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        \"\"\"Creates a normal distribution conditioned on the inputs.\"\"\"\n",
    "        logits = self.get_logits(args, **kwargs)\n",
    "        return tfp.distributions.Bernoulli(logits=logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VRNNTransitionModel(TransitionModelBase):\n",
    "    def __init__(self, \n",
    "                 rnn_hidden_size, \n",
    "                 data_encoder, \n",
    "                 latent_encoder,\n",
    "                 name='NNTransitionModel'):\n",
    "        \n",
    "        super(VRNNTransitionModel, self).__init__(name=name)\n",
    "        \n",
    "        # mlp parametrised gaussian\n",
    "        self.transition = NNNormalDistribution(size=latent_size, \n",
    "                                               hidden_layer_sizes=[latent_size])\n",
    "        # encoder for inputs\n",
    "        self.latent_encoder = latent_encoder\n",
    "        \n",
    "        # lstm cell\n",
    "        self.rnn_hidden_size = rnn_hidden_size\n",
    "        self.rnn = tf.keras.layers.LSTMCell(rnn_hidden_size)\n",
    "        \n",
    "    def run_rnn(self, state: State, inputs: tf.Tensor):\n",
    "        \n",
    "        tiled_inputs = tf.tile(inputs, [state.batch_size, state.n_particles, 1])\n",
    "        # process latent state\n",
    "        latent_state = state.particles\n",
    "        \n",
    "        # encode and reshape latent state\n",
    "        latent_encoded = self.latent_encoder(latent_state)\n",
    "        \n",
    "        B, N, D = latent_encoded.shape\n",
    "        # process rnn_state\n",
    "        rnn_state = tf.reshape(state.rnn_state, [B,  N, self.rnn_hidden_size*2])\n",
    "\n",
    "        rnn_state = tf.split(rnn_state, 2, axis=-1)\n",
    "        \n",
    "\n",
    "        # run rnn\n",
    "        rnn_inputs = tf.concat([tiled_inputs, latent_encoded], axis=-1)\n",
    "        rnn_inputs_reshaped = tf.reshape(rnn_inputs, (B*N, -1))\n",
    "        rnn_state_reshaped = [tf.reshape(elem, (B*N, -1)) for elem in rnn_state]\n",
    "        rnn_out, rnn_state = self.rnn(rnn_inputs_reshaped, rnn_state_reshaped)\n",
    "\n",
    "\n",
    "        rnn_state = tf.concat(rnn_state, axis=-1)\n",
    "        rnn_state = tf.reshape(rnn_state, [state.batch_size, state.n_particles, self.rnn_hidden_size*2])\n",
    "        rnn_out = tf.reshape(rnn_out, [state.batch_size, state.n_particles, self.rnn_hidden_size])\n",
    "        return rnn_out, rnn_state, latent_encoded\n",
    "    \n",
    "    def latent_dist(self, state, rnn_out):\n",
    "        dist = self.transition(rnn_out)\n",
    "        return dist\n",
    "\n",
    "    def loglikelihood(self, prior_state: State, proposed_state: State, inputs: tf.Tensor):\n",
    "        rnn_out, rnn_state, latent_encoded = self.run_rnn(prior_state, inputs)\n",
    "        dist = self.transition(rnn_out)\n",
    "        new_latent = proposed_state.particles\n",
    "        return tf.reduce_sum(dist.log_prob(new_latent), axis=-1)\n",
    "\n",
    "    def sample(self, state: State, inputs: tf.Tensor, seed=None):\n",
    "        \n",
    "        rnn_out, rnn_state, latent_encoded = self.run_rnn(state, inputs)\n",
    "        dist = self.latent_dist(state, rnn_out)\n",
    "        latent_state = dist.sample(seed=seed)\n",
    "        \n",
    "        return VRNNState(particles=latent_state, \n",
    "                          log_weights = state.log_weights,\n",
    "                          weights=state.weights, \n",
    "                          log_likelihoods=state.log_likelihoods,\n",
    "                          rnn_state = rnn_state,\n",
    "                          rnn_out = rnn_out,\n",
    "                          latent_encoded =  latent_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VRNNProposalModel(VRNNTransitionModel):\n",
    "    def __init__(self, \n",
    "                 rnn_hidden_size, \n",
    "                 data_encoder, \n",
    "                 latent_encoder,\n",
    "                 name='VRNNProposalModel'):\n",
    "        \n",
    "        super(VRNNProposalModel, self).__init__(rnn_hidden_size, \n",
    "                 data_encoder, \n",
    "                 latent_encoder,name)\n",
    "\n",
    "    def loglikelihood(self, proposed_state: State, state: State, inputs: tf.Tensor, observation: tf.Tensor):\n",
    "        rnn_out, rnn_state, latent_encoded = self.run_rnn(state, inputs)\n",
    "        dist = self.latent_dist(state, rnn_out)\n",
    "        new_latent = proposed_state.particles\n",
    "        return tf.reduce_sum(dist.log_prob(new_latent), axis=-1)\n",
    "    \n",
    "    def propose(self, state: State, inputs: tf.Tensor, observation: tf.Tensor, seed=None):\n",
    "        return self.sample(state, inputs, seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VRNNBernoulliObservationModel(ObservationSampler):\n",
    "    \n",
    "    def __init__(self, latent_encoder, observation_size, name='VRNNObservationModel'):\n",
    "        super(VRNNBernoulliObservationModel, self).__init__(name=name)\n",
    "        # mlp parametrised gaussian\n",
    "        self.emission = NNBernoulliDistribution(size=observation_size, \n",
    "                                                hidden_layer_sizes=[observation_size])\n",
    "        \n",
    "    \n",
    "    def observation_dist(self, state: State):\n",
    "        latent_state = state.particles\n",
    "        latent_encoded = state.latent_encoded\n",
    "        rnn_out = state.rnn_out\n",
    "        dist = self.emission(latent_encoded, rnn_out)\n",
    "        return dist\n",
    "    \n",
    "    def loglikelihood(self, state: State, observation: tf.Tensor):\n",
    "        dist = self.observation_dist(state)\n",
    "        return tf.reduce_sum(dist.log_prob(observation), axis=-1)\n",
    "        \n",
    "\n",
    "    def sample(self, state: State):\n",
    "        dist = self.observation_dist(state)\n",
    "        return dist.sample()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@attr.s\n",
    "class VRNNState(State):\n",
    "    ADDITIONAL_STATE_VARIABLES = ('rnn_state',) # rnn_out and encoded no need to be resampled\n",
    "    rnn_state = attr.ib(default=None)\n",
    "    rnn_out = attr.ib(default=None)\n",
    "    latent_encoded = attr.ib(default=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_model = VRNNTransitionModel(rnn_hidden_size, data_encoder, latent_encoder)\n",
    "observation_model = VRNNBernoulliObservationModel(latent_encoder, observation_size)\n",
    "proposal_model = VRNNProposalModel(rnn_hidden_size, data_encoder, latent_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial state\n",
    "normal_dist = tfp.distributions.Normal(0., 1.)\n",
    "initial_latent_state = tf.zeros([batch_size, n_particles, dimension])\n",
    "initial_latent_state = tf.cast(initial_latent_state, dtype=float)\n",
    "latent_encoded = transition_model.latent_encoder(initial_latent_state)\n",
    "\n",
    "# initial rnn_state\n",
    "initial_rnn_state = [normal_dist.sample([batch_size,n_particles,rnn_hidden_size])]*2\n",
    "initial_rnn_state = tf.concat(initial_rnn_state, axis=-1)\n",
    "\n",
    "# rnn_out\n",
    "initial_rnn_out = tf.zeros([batch_size, n_particles, rnn_hidden_size])\n",
    "\n",
    "initial_weights = tf.ones((batch_size, n_particles), dtype=float) / tf.cast(n_particles, float)\n",
    "log_likelihoods = tf.zeros(batch_size, dtype=float)\n",
    "initial_state = VRNNState(particles=initial_latent_state, \n",
    "                          log_weights = tf.math.log(initial_weights),\n",
    "                          weights=initial_weights, \n",
    "                          log_likelihoods=log_likelihoods,\n",
    "                          rnn_state = initial_rnn_state,\n",
    "                          rnn_out = initial_rnn_out,\n",
    "                          latent_encoded = latent_encoded)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 1, 88])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_tensor[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# snt networks initiated on first call\n",
    "t_samp = transition_model.sample(initial_state, inputs_tensor[0])\n",
    "obs_samp = observation_model.sample(initial_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latent_encoder/linear_0/b:0\n",
      "latent_encoder/linear_0/w:0\n",
      "latent_encoder/linear_1/b:0\n",
      "latent_encoder/linear_1/w:0\n",
      "lstm_cell/kernel:0\n",
      "lstm_cell/recurrent_kernel:0\n",
      "lstm_cell/bias:0\n",
      "conditional_normal_distribution_fcnet/linear_0/b:0\n",
      "conditional_normal_distribution_fcnet/linear_0/w:0\n",
      "conditional_normal_distribution_fcnet/linear_1/b:0\n",
      "conditional_normal_distribution_fcnet/linear_1/w:0\n"
     ]
    }
   ],
   "source": [
    "for var in transition_model.variables:\n",
    "    print(var.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conditional_bernoulli_distribution_fcnet/linear_0/b:0\n",
      "conditional_bernoulli_distribution_fcnet/linear_0/w:0\n",
      "conditional_bernoulli_distribution_fcnet/linear_1/b:0\n",
      "conditional_bernoulli_distribution_fcnet/linear_1/w:0\n"
     ]
    }
   ],
   "source": [
    "for var in observation_model.variables:\n",
    "    print(var.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Particle Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable_variables = transition_model.variables + observation_model.variables\n",
    "init_values = [v.value() for v in trainable_variables]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = n_particles\n",
    "B = batch_size\n",
    "\n",
    "# initial state\n",
    "normal_dist = tfp.distributions.Normal(0., 1.)\n",
    "initial_latent_state = tf.zeros([B, N, dimension])\n",
    "initial_latent_state = tf.cast(initial_latent_state, dtype=float)\n",
    "latent_encoded = transition_model.latent_encoder(initial_latent_state)\n",
    "\n",
    "# initial rnn_state\n",
    "initial_rnn_state = [normal_dist.sample([B, N, rnn_hidden_size], seed=filter_seed)]*2\n",
    "initial_rnn_state = tf.concat(initial_rnn_state, axis=-1)\n",
    "\n",
    "# rnn_out\n",
    "initial_rnn_out = tf.zeros([B, N, rnn_hidden_size])\n",
    "\n",
    "initial_weights = tf.ones((B, N), dtype=float) / tf.cast(N, float)\n",
    "log_likelihoods = tf.zeros(B, dtype=float)\n",
    "\n",
    "init_state = VRNNState(particles=initial_latent_state, \n",
    "                          log_weights = tf.math.log(initial_weights),\n",
    "                          weights=initial_weights, \n",
    "                          log_likelihoods=log_likelihoods,\n",
    "                          rnn_state=initial_rnn_state,\n",
    "                          rnn_out=initial_rnn_out,\n",
    "                          latent_encoded=latent_encoded)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "resampling_criterion = NeffCriterion(tf.constant(0.9), tf.constant(True), verbose=False)\n",
    "#resampling_criterion = AlwaysResample()\n",
    "resampling_method = MultinomialResampler()\n",
    "\n",
    "epsilon = tf.constant(0.5)\n",
    "scaling = tf.constant(0.9)\n",
    "\n",
    "regularized = RegularisedTransform(epsilon, scaling=scaling, max_iter=1000, convergence_threshold=1e-3)\n",
    "\n",
    "    \n",
    "multinomial_smc = SMC(observation_model, transition_model, proposal_model, resampling_criterion, resampling_method)\n",
    "regularized_smc = SMC(observation_model, transition_model, proposal_model, resampling_criterion, regularized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "initial_learning_rate = 0.01\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate,\n",
    "    decay_steps=100,\n",
    "    decay_rate=0.9,\n",
    "    staircase=True)\n",
    "\n",
    "optimizer = tf.optimizers.Adam(learning_rate=lr_schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def smc_routine(smc, state, use_correction_term=False, seed=filter_seed):\n",
    "    final_state = smc(state, obs_data, n_observations=T, inputs_series=inputs_data, return_final=True, seed=filter_seed)\n",
    "    res = tf.reduce_mean(final_state.log_likelihoods)\n",
    "    if use_correction_term:\n",
    "        return res, tf.reduce_mean(final_state.resampling_correction)\n",
    "    return res, tf.constant(0.)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def run_one_step(smc, use_correction_term, init_state):\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(trainable_variables)\n",
    "        real_ll, correction = smc_routine(smc, init_state, use_correction_term)\n",
    "        loss = -(real_ll + correction)\n",
    "    grads_loss = tape.gradient(loss, trainable_variables)\n",
    "    return real_ll, grads_loss\n",
    "\n",
    "@tf.function\n",
    "def train_one_step(smc, use_correction_term):\n",
    "    real_ll, grads_loss = run_one_step(smc, use_correction_term, init_state)\n",
    "    optimizer.apply_gradients(zip(grads_loss, trainable_variables))\n",
    "    return -real_ll, grads_loss\n",
    "\n",
    "@tf.function\n",
    "def train_niter(smc, num_steps=100, use_correction_term=False, reset=True):\n",
    "    if reset:\n",
    "        reset_operations = [v.assign(init) for v, init in zip(trainable_variables, init_values)]\n",
    "    else:\n",
    "        reset_operations = []\n",
    "    loss_tensor_array = tf.TensorArray(dtype=tf.float32, size=num_steps, dynamic_size=False, element_shape=[])\n",
    "    grad_tensor_array = tf.TensorArray(dtype=tf.float32, size=num_steps, dynamic_size=False, element_shape=[])\n",
    "    time_tensor_array = tf.TensorArray(dtype=tf.float64, size=num_steps, dynamic_size=False, element_shape=[])\n",
    "    with tf.control_dependencies(reset_operations):\n",
    "        toc = tf.constant(0., dtype=tf.float64)\n",
    "        tic = tf.timestamp()\n",
    "        for step in tf.range(1, num_steps+1):\n",
    "            \n",
    "            tic_loss = tf.timestamp()\n",
    "            with tf.control_dependencies([tic_loss]):\n",
    "                loss, grads = train_one_step(smc, use_correction_term)\n",
    "            with tf.control_dependencies([loss]):\n",
    "                toc_loss = tf.timestamp()            \n",
    "\n",
    "\n",
    "            toc += toc_loss - tic_loss\n",
    "            \n",
    "            max_grad = tf.reduce_max([tf.reduce_max(tf.abs(grad)) for grad in grads])\n",
    "            \n",
    "            tf.print('Step', step, '/', num_steps, ': ms per step= ', 1000. * toc / tf.cast(step, tf.float64), ': total compute time (s)= ', toc, 'Real Time elapsed (s): ', tf.timestamp()-tic, ', loss = ', loss, ', max abs grads = ', max_grad, end='\\r')\n",
    "            \n",
    "            loss_tensor_array = loss_tensor_array.write(step-1, -loss)\n",
    "            grad_tensor_array = grad_tensor_array.write(step-1, max_grad)\n",
    "            time_tensor_array = time_tensor_array.write(step-1, toc)\n",
    "    return loss_tensor_array.stack(), grad_tensor_array.stack(), time_tensor_array.stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs\n",
      "Tensor(\"IteratorGetNext_1:0\", shape=(1, 1, 88), dtype=float32)\n",
      "obs\n",
      "Tensor(\"IteratorGetNext:0\", shape=(1, 1, 88), dtype=float32)\n",
      "WARNING:tensorflow:From /data/hylia/thornton/venvs/filter_venv/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Step 100 / 100 : ms per step=  178.11030864715576 : total compute time (s)=  17.811030864715576 Real Time elapsed (s):  17.8863742351532 , loss =  926.973389 , max abs grads =  840.46667534\r"
     ]
    }
   ],
   "source": [
    "multinomial_ll_n_epochs, _, multinomial_time = train_niter(multinomial_smc, tf.constant(n_iter))\n",
    "multinomial_ll_n_epochs_numpy = multinomial_ll_n_epochs.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer.set_weights([w * 0 for w in optimizer.get_weights()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs\n",
      "Tensor(\"IteratorGetNext_1:0\", shape=(1, 1, 88), dtype=float32)\n",
      "obs\n",
      "Tensor(\"IteratorGetNext:0\", shape=(1, 1, 88), dtype=float32)\n",
      "Step 16 / 100 : ms per step=  1694.354459643364 : total compute time (s)=  27.109671354293823 Real Time elapsed (s):  27.122084856033325 , loss =  1692.68469 , max abs grads =  120.0410549\r"
     ]
    }
   ],
   "source": [
    "reg_ll_n_epochs, _, reg_time = train_niter(regularized_smc, tf.constant(n_iter))\n",
    "reg_ll_n_epochs_numpy = reg_ll_n_epochs.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(multinomial_ll_n_epochs_numpy, color='blue')\n",
    "ax.plot(reg_ll_n_epochs_numpy, color='green')\n",
    "fig.savefig(os.path.join('./charts/', 'vrnn_loss_per_epoch.png'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warmup = 100\n",
    "end = 500\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(multinomial_ll_n_epochs_numpy[warmup:end], color='blue')\n",
    "ax.plot(reg_ll_n_epochs_numpy[warmup:end], color='green')\n",
    "fig.savefig(os.path.join('./charts/', 'vrnn_loss_per_epoch.png'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "filter_venv",
   "language": "python",
   "name": "filter_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
