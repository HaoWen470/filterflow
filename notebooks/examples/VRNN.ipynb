{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "# add to path\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "import attr\n",
    "import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import seaborn\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "import pandas as pd\n",
    "import sonnet as snt\n",
    "\n",
    "from filterflow.smc import SMC\n",
    "from filterflow.base import State, StateWithMemory, StateSeries, DTYPE_TO_OBSERVATION_SERIES, DTYPE_TO_STATE_SERIES\n",
    "\n",
    "from filterflow.observation.base import ObservationModelBase, ObservationSampler\n",
    "from filterflow.observation.linear import LinearObservationSampler\n",
    "from filterflow.transition.random_walk import RandomWalkModel\n",
    "from filterflow.proposal import BootstrapProposalModel\n",
    "from filterflow.transition.base import TransitionModelBase\n",
    "\n",
    "from filterflow.resampling.criterion import NeffCriterion, AlwaysResample, NeverResample\n",
    "from filterflow.resampling.standard import SystematicResampler, MultinomialResampler\n",
    "from filterflow.resampling.differentiable import RegularisedTransform, CorrectedRegularizedTransform\n",
    "from filterflow.resampling.differentiable.ricatti.solver import RicattiSolver\n",
    "\n",
    "from filterflow.resampling.base import NoResampling\n",
    "\n",
    "from filterflow.state_space_model import StateSpaceModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "from scipy.sparse import coo_matrix\n",
    "data_dir = \"../../data/piano_data\"\n",
    "\n",
    "def sparse_pianoroll_to_dense(pianoroll, min_note, num_notes):\n",
    "    \"\"\"Converts a sparse pianoroll to a dense numpy array.\n",
    "    Given a sparse pianoroll, converts it to a dense numpy array of shape\n",
    "    [num_timesteps, num_notes] where entry i,j is 1.0 if note j is active on\n",
    "    timestep i and 0.0 otherwise.\n",
    "    Args:\n",
    "    pianoroll: A sparse pianoroll object, a list of tuples where the i'th tuple\n",
    "      contains the indices of the notes active at timestep i.\n",
    "    min_note: The minimum note in the pianoroll, subtracted from all notes so\n",
    "      that the minimum note becomes 0.\n",
    "    num_notes: The number of possible different note indices, determines the\n",
    "      second dimension of the resulting dense array.\n",
    "    Returns:\n",
    "    dense_pianoroll: A [num_timesteps, num_notes] numpy array of floats.\n",
    "    num_timesteps: A python int, the number of timesteps in the pianoroll.\n",
    "    \"\"\"\n",
    "    num_timesteps = len(pianoroll)\n",
    "    inds = []\n",
    "    for time, chord in enumerate(pianoroll):\n",
    "        # Re-index the notes to start from min_note.\n",
    "        inds.extend((time, note-min_note) for note in chord)\n",
    "        shape = [num_timesteps, num_notes]\n",
    "    values = [1.] * len(inds)\n",
    "    sparse_pianoroll = coo_matrix(\n",
    "      (values, ([x[0] for x in inds], [x[1] for x in inds])),\n",
    "      shape=shape)\n",
    "    return sparse_pianoroll.toarray(), num_timesteps\n",
    "\n",
    "def create_pianoroll_dataset(path,\n",
    "                             split,\n",
    "                             batch_size,\n",
    "                             num_parallel_calls=4,\n",
    "                             shuffle=False,\n",
    "                             repeat=False,\n",
    "                             min_note=21,\n",
    "                             max_note=108):\n",
    "    \"\"\"Creates a pianoroll dataset.\n",
    "    Args:\n",
    "    path: The path of a pickle file containing the dataset to load.\n",
    "    split: The split to use, can be train, test, or valid.\n",
    "    batch_size: The batch size. If repeat is False then it is not guaranteed\n",
    "      that the true batch size will match for all batches since batch_size\n",
    "      may not necessarily evenly divide the number of elements.\n",
    "    num_parallel_calls: The number of threads to use for parallel processing of\n",
    "      the data.\n",
    "    shuffle: If true, shuffles the order of the dataset.\n",
    "    repeat: If true, repeats the dataset endlessly.\n",
    "    min_note: The minimum note number of the dataset. For all pianoroll datasets\n",
    "      the minimum note is number 21, and changing this affects the dimension of\n",
    "      the data. This is useful mostly for testing.\n",
    "    max_note: The maximum note number of the dataset. For all pianoroll datasets\n",
    "      the maximum note is number 108, and changing this affects the dimension of\n",
    "      the data. This is useful mostly for testing.\n",
    "    Returns:\n",
    "    inputs: A batch of input sequences represented as a dense Tensor of shape\n",
    "      [time, batch_size, data_dimension]. The sequences in inputs are the\n",
    "      sequences in targets shifted one timestep into the future, padded with\n",
    "      zeros. This tensor is mean-centered, with the mean taken from the pickle\n",
    "      file key 'train_mean'.\n",
    "    targets: A batch of target sequences represented as a dense Tensor of\n",
    "      shape [time, batch_size, data_dimension].\n",
    "    lens: An int Tensor of shape [batch_size] representing the lengths of each\n",
    "      sequence in the batch.\n",
    "    mean: A float Tensor of shape [data_dimension] containing the mean loaded\n",
    "      from the pickle file.\n",
    "    \"\"\"\n",
    "    # Load the data from disk.\n",
    "    num_notes = max_note - min_note + 1\n",
    "    with tf.io.gfile.GFile(path, \"rb\") as f:\n",
    "        raw_data = pickle.load(f)\n",
    "    pianorolls = raw_data[split]\n",
    "    mean = raw_data[\"train_mean\"]\n",
    "    num_examples = len(pianorolls)\n",
    "\n",
    "    def pianoroll_generator():\n",
    "        for sparse_pianoroll in pianorolls:\n",
    "            yield sparse_pianoroll_to_dense(sparse_pianoroll, min_note, num_notes)\n",
    "\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "      pianoroll_generator,\n",
    "      output_types=(tf.float64, tf.int64),\n",
    "      output_shapes=([None, num_notes], []))\n",
    "\n",
    "    if repeat: \n",
    "        dataset = dataset.repeat()\n",
    "    if shuffle: \n",
    "        dataset = dataset.shuffle(num_examples)\n",
    "\n",
    "    # Batch sequences togther, padding them to a common length in time.\n",
    "    dataset = dataset.padded_batch(batch_size,\n",
    "                                 padded_shapes=([None, num_notes], []))\n",
    "\n",
    "    def process_pianoroll_batch(data, lengths):\n",
    "        \"\"\"Create mean-centered and time-major next-step prediction Tensors.\"\"\"\n",
    "        data = tf.cast(tf.transpose(data, perm=[1, 0, 2]), float)\n",
    "        lengths = tf.cast(lengths, tf.int32)\n",
    "        targets = data\n",
    "        # Mean center the inputs.\n",
    "        inputs = data - tf.constant(mean, dtype=tf.float32,\n",
    "                                    shape=[1, 1, mean.shape[0]])\n",
    "        # Shift the inputs one step forward in time. Also remove the last timestep\n",
    "        # so that targets and inputs are the same length.\n",
    "        inputs = tf.pad(inputs, [[1, 0], [0, 0], [0, 0]], mode=\"CONSTANT\")[:-1]\n",
    "        # Mask out unused timesteps.\n",
    "        inputs *= tf.expand_dims(tf.transpose(\n",
    "            tf.sequence_mask(lengths, dtype=inputs.dtype)), 2)\n",
    "        return inputs, targets, lengths\n",
    "\n",
    "    dataset = dataset.map(process_pianoroll_batch,\n",
    "                        num_parallel_calls=num_parallel_calls)\n",
    "    dataset = dataset.prefetch(num_examples)\n",
    "\n",
    "    itr = tf.compat.v1.data.make_one_shot_iterator(dataset)\n",
    "    inputs, targets, lengths = itr.get_next()\n",
    "    return inputs, targets, lengths, tf.constant(mean, dtype=tf.float32)\n",
    "\n",
    "path = os.path.join(data_dir, 'jsb.pkl')\n",
    "inputs, targets, lens, mean = create_pianoroll_dataset(path, split='train', batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "T = targets.shape.as_list()[0]\n",
    "observation_size = targets.shape.as_list()[-1]\n",
    "\n",
    "\n",
    "latent_size = 10\n",
    "fcnet_hidden_sizes = [latent_size]\n",
    "encoded_data_size = latent_size\n",
    "rnn_hidden_size = latent_size//2\n",
    "\n",
    "latent_encoder_layers = [32]\n",
    "latent_encoded_size = 32\n",
    "\n",
    "latent_encoder = snt.nets.MLP(\n",
    "            output_sizes=latent_encoder_layers + [latent_encoded_size],\n",
    "            name=\"latent_encoder\")\n",
    "\n",
    "data_encoder_layers = [32]\n",
    "encoded_data_size = 32\n",
    "data_encoder = snt.nets.MLP(\n",
    "            output_sizes=data_encoder_layers + [encoded_data_size],\n",
    "            name=\"data_encoder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store observations\n",
    "batch_size = 1\n",
    "n_particles = 100\n",
    "dimension = latent_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.tile(inputs, [1, batch_size*n_particles, 1])\n",
    "targets = tf.tile(targets, [1, batch_size*n_particles, 1])\n",
    "\n",
    "obs_data = tf.data.Dataset.from_tensor_slices(targets)\n",
    "inputs_data = tf.data.Dataset.from_tensor_slices(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNNormalDistribution(tf.Module):\n",
    "    \"\"\"A Normal distribution with mean and var parametrised by NN\"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 size, \n",
    "                 hidden_layer_sizes, \n",
    "                 sigma_min=0.0,\n",
    "                 raw_sigma_bias=0.25, \n",
    "                 hidden_activation_fn=tf.nn.relu,\n",
    "                 name=\"conditional_normal_distribution\"):\n",
    "        \n",
    "        super(NNNormalDistribution, self).__init__(name=name)\n",
    "        \n",
    "        self.sigma_min = sigma_min\n",
    "        self.raw_sigma_bias = raw_sigma_bias\n",
    "        self.size = size\n",
    "        self.fcnet = snt.nets.MLP(\n",
    "            output_sizes=hidden_layer_sizes + [2*size],\n",
    "            activation=hidden_activation_fn,\n",
    "            activate_final=False,\n",
    "            name=name + \"_fcnet\")\n",
    "\n",
    "    def get_params(self, tensor_list, **unused_kwargs):\n",
    "        \"\"\"Computes the parameters of a normal distribution based on the inputs.\"\"\"\n",
    "        inputs = tf.concat(tensor_list, axis=-1)\n",
    "        outs = self.fcnet(inputs)\n",
    "        mu, sigma = tf.split(outs, 2, axis=-1)\n",
    "        sigma = tf.maximum(tf.nn.softplus(sigma + self.raw_sigma_bias), self.sigma_min)\n",
    "        return mu, sigma\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        \"\"\"Creates a normal distribution conditioned on the inputs.\"\"\"\n",
    "        mu, sigma = self.get_params(args, **kwargs)\n",
    "        return tfp.distributions.Normal(loc=mu, scale=sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VRNNTransitionModel(TransitionModelBase):\n",
    "    def __init__(self, \n",
    "                 rnn_hidden_size, \n",
    "                 data_encoder, \n",
    "                 latent_encoder,\n",
    "                 name='NNTransitionModel'):\n",
    "        \n",
    "        super(VRNNTransitionModel, self).__init__(name=name)\n",
    "        \n",
    "        # mlp parametrised gaussian\n",
    "        self.transition = NNNormalDistribution(size=latent_size, \n",
    "                                               hidden_layer_sizes=[latent_size])\n",
    "        # encoder for inputs\n",
    "        self.latent_encoder = latent_encoder\n",
    "        \n",
    "        # lstm cell\n",
    "        self.rnn_hidden_size = rnn_hidden_size\n",
    "        self.lstm_cell = tf.keras.layers.LSTMCell(units=rnn_hidden_size)\n",
    "        \n",
    "    def run_rnn(self, state: State, inputs: tf.Tensor):\n",
    "        \n",
    "        # process latent state\n",
    "        latent_state = state.particles\n",
    "        \n",
    "        # encode and reshape latent state\n",
    "        latent_encoded = self.latent_encoder(latent_state)\n",
    "        \n",
    "        B,N, D = latent_encoded.shape\n",
    "        latent_encoded_reshaped = tf.reshape(latent_encoded, [B*N,D])\n",
    "        \n",
    "        # process rnn_state\n",
    "        rnn_state = tf.reshape(state.rnn_state, [B*N,rnn_hidden_size*2])\n",
    "        rnn_state = tf.split(rnn_state, 2, axis=-1)\n",
    "        \n",
    "        \n",
    "        # run rnn\n",
    "        rnn_inputs = tf.concat([inputs, latent_encoded_reshaped], axis=-1)\n",
    "        rnn_out, rnn_state = self.lstm_cell(rnn_inputs, rnn_state)\n",
    "        \n",
    "        rnn_state = tf.concat(rnn_state, axis=-1)\n",
    "        rnn_state = tf.reshape(rnn_state, [state.batch_size, state.n_particles, rnn_hidden_size*2])\n",
    "        return rnn_out, rnn_state, latent_encoded\n",
    "    \n",
    "    def latent_dist(self, rnn_out):\n",
    "        rnn_out = tf.reshape(rnn_out, [state.batch_size, state.n_particles, rnn_hidden_size])\n",
    "        dist = self.transition(rnn_out)\n",
    "        return dist\n",
    "\n",
    "    def loglikelihood(self, prior_state: State, proposed_state: State, inputs: tf.Tensor):\n",
    "        rnn_out, rnn_state, latent_encoded = self.run_rnn(state, inputs)\n",
    "        dist = self.latent_dist(rnn_out)\n",
    "        new_latent = proposed_state.particles\n",
    "        return tf.reduce_sum(dist.log_prob(new_latent), axis=-1)\n",
    "\n",
    "    def sample(self, state: State, inputs: tf.Tensor):\n",
    "        \n",
    "        rnn_out, rnn_state, latent_encoded = self.run_rnn(state, inputs)\n",
    "        dist = self.latent_dist(rnn_out)\n",
    "        latent_state = dist.sample()\n",
    "        \n",
    "        return VRNNState(particles=latent_state, \n",
    "                          log_weights = state.log_weights,\n",
    "                          weights=state.weights, \n",
    "                          log_likelihoods=state.log_likelihoods,\n",
    "                          rnn_state = rnn_state,\n",
    "                          rnn_out = rnn_out,\n",
    "                          latent_encoded =  latent_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VRNNProposalModel(VRNNTransitionModel):\n",
    "    def __init__(self, \n",
    "                 rnn_hidden_size, \n",
    "                 data_encoder, \n",
    "                 latent_encoder,\n",
    "                 name='VRNNProposalModel'):\n",
    "        \n",
    "        super(VRNNProposalModel, self).__init__(rnn_hidden_size, \n",
    "                 data_encoder, \n",
    "                 latent_encoder,name)\n",
    "\n",
    "    def loglikelihood(self, proposed_state: State, state: State, inputs: tf.Tensor, observation: tf.Tensor):\n",
    "        rnn_out, rnn_state, latent_encoded = self.run_rnn(state, inputs)\n",
    "        dist = self.latent_dist(rnn_out)\n",
    "        new_latent = proposed_state.particles\n",
    "        return tf.reduce_sum(dist.log_prob(new_latent), axis=-1)\n",
    "    \n",
    "    def propose(self, state: State, inputs: tf.Tensor, observation: tf.Tensor):\n",
    "        return self.sample(state, inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VRNNNormalObservationModel(ObservationSampler):\n",
    "    \n",
    "    def __init__(self, latent_encoder, observation_size, name='VRNNObservationModel'):\n",
    "        super(VRNNNormalObservationModel, self).__init__(name=name)\n",
    "        # mlp parametrised gaussian\n",
    "        self.emission = NNNormalDistribution(size=observation_size, \n",
    "                                             hidden_layer_sizes=[observation_size])\n",
    "        \n",
    "    \n",
    "    def observation_dist(self, state: State):\n",
    "        latent_state = state.particles\n",
    "        latent_encoded = state.latent_encoded\n",
    "        rnn_out = state.rnn_out\n",
    "        rnn_out = tf.reshape(rnn_out, [state.batch_size, state.n_particles, rnn_hidden_size])\n",
    "        dist = self.emission(latent_state, rnn_out) \n",
    "        return dist\n",
    "    \n",
    "    def loglikelihood(self, state: State, observation: tf.Tensor):\n",
    "        \n",
    "        dist = self.observation_dist(state)\n",
    "        return tf.reduce_sum(dist.log_prob(observation), axis=-1)\n",
    "        \n",
    "\n",
    "    def sample(self, state: State):\n",
    "        dist = self.observation_dist(state)\n",
    "        \n",
    "        \n",
    "        return dist.sample()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VRNNBernoulliObservationModel(ObservationSampler):\n",
    "    \n",
    "    def __init__(self, latent_encoder, observation_size, name='VRNNObservationModel'):\n",
    "        super(VRNNNormalObservationModel, self).__init__(name=name)\n",
    "        # mlp parametrised gaussian\n",
    "        self.emission = NNNormalDistribution(size=observation_size, \n",
    "                                             hidden_layer_sizes=[observation_size])\n",
    "        \n",
    "    \n",
    "    def observation_dist(self, state: State):\n",
    "        latent_state = state.particles\n",
    "        latent_encoded = state.latent_encoded\n",
    "        rnn_out = state.rnn_out\n",
    "        rnn_out = tf.reshape(rnn_out, [state.batch_size, state.n_particles, rnn_hidden_size])\n",
    "        dist = self.emission(latent_state, rnn_out) \n",
    "        return dist\n",
    "    \n",
    "    def loglikelihood(self, state: State, observation: tf.Tensor):\n",
    "        \n",
    "        dist = self.observation_dist(state)\n",
    "        return tf.reduce_sum(dist.log_prob(observation), axis=-1)\n",
    "        \n",
    "\n",
    "    def sample(self, state: State):\n",
    "        dist = self.observation_dist(state)\n",
    "        \n",
    "        \n",
    "        return dist.sample()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@attr.s\n",
    "class VRNNState(State):\n",
    "    ADDITIONAL_STATE_VARIABLES = ('rnn_state',) # rnn_out and encoded no need o be resampled\n",
    "    rnn_state = attr.ib()\n",
    "    rnn_out = attr.ib()\n",
    "    latent_encoded = attr.ib()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_model = VRNNTransitionModel(rnn_hidden_size, data_encoder, latent_encoder)\n",
    "observation_model = VRNNNormalObservationModel(latent_encoder, observation_size)\n",
    "proposal_model = VRNNProposalModel(rnn_hidden_size, data_encoder, latent_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial state\n",
    "normal_dist = tfp.distributions.Normal(0., 1.)\n",
    "initial_latent_state = tf.zeros([batch_size, n_particles, dimension])\n",
    "initial_latent_state = tf.cast(initial_latent_state, dtype=float)\n",
    "latent_encoded = transition_model.latent_encoder(initial_latent_state)\n",
    "\n",
    "# initial rnn_state\n",
    "initial_rnn_state = [normal_dist.sample([batch_size,n_particles,rnn_hidden_size])]*2\n",
    "initial_rnn_state = tf.concat(initial_rnn_state, axis=-1)\n",
    "\n",
    "# rnn_out\n",
    "initial_rnn_out = tf.zeros([batch_size * n_particles, rnn_hidden_size])\n",
    "\n",
    "initial_weights = tf.ones((batch_size, n_particles), dtype=float) / tf.cast(n_particles, float)\n",
    "log_likelihoods = tf.zeros(batch_size, dtype=float)\n",
    "initial_state = VRNNState(particles=initial_latent_state, \n",
    "                          log_weights = tf.math.log(initial_weights),\n",
    "                          weights=initial_weights, \n",
    "                          log_likelihoods=log_likelihoods,\n",
    "                          rnn_state = initial_rnn_state,\n",
    "                          rnn_out = initial_rnn_out,\n",
    "                          latent_encoded = latent_encoded)\n",
    "\n",
    "\n",
    "state = initial_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# snt networks initiated on first call\n",
    "t_samp = transition_model.sample(state, inputs[0])\n",
    "obs_samp = observation_model.sample(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latent_encoder/linear_0/b:0\n",
      "latent_encoder/linear_0/w:0\n",
      "latent_encoder/linear_1/b:0\n",
      "latent_encoder/linear_1/w:0\n",
      "lstm_cell/kernel:0\n",
      "lstm_cell/recurrent_kernel:0\n",
      "lstm_cell/bias:0\n",
      "conditional_normal_distribution_fcnet/linear_0/b:0\n",
      "conditional_normal_distribution_fcnet/linear_0/w:0\n",
      "conditional_normal_distribution_fcnet/linear_1/b:0\n",
      "conditional_normal_distribution_fcnet/linear_1/w:0\n"
     ]
    }
   ],
   "source": [
    "for var in transition_model.variables:\n",
    "    print(var.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conditional_normal_distribution_fcnet/linear_0/b:0\n",
      "conditional_normal_distribution_fcnet/linear_0/w:0\n",
      "conditional_normal_distribution_fcnet/linear_1/b:0\n",
      "conditional_normal_distribution_fcnet/linear_1/w:0\n"
     ]
    }
   ],
   "source": [
    "for var in observation_model.variables:\n",
    "    print(var.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Particle Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resampling\n",
    "resampling_criterion = NeffCriterion(tf.constant(0.5), tf.constant(True))\n",
    "systematic = SystematicResampler()\n",
    "multinomial = MultinomialResampler()\n",
    "\n",
    "\n",
    "epsilon = tf.constant(0.25)\n",
    "scaling = tf.constant(0.9)\n",
    "\n",
    "regularized = RegularisedTransform(epsilon, scaling=scaling, max_iter=1000, convergence_threshold=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "particle_filter = SMC(observation_model, transition_model, proposal_model, resampling_criterion, regularized)\n",
    "\n",
    "recorded_states = particle_filter(initial_state, observation_series=obs_data, n_observations=T, inputs_series= inputs_data)\n",
    "\n",
    "recorded_states  = attr.evolve(recorded_states)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
