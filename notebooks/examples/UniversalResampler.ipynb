{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import bqplot.pyplot as bplt\n",
    "from ipywidgets import HBox\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "from filterflow.resampling.differentiable.loss.regularized import SinkhornLoss\n",
    "from filterflow.resampling.differentiable.regularized_transport.utils import cost\n",
    "from filterflow.resampling.differentiable.biased import RegularisedTransform\n",
    "from filterflow.resampling.standard.systematic import SystematicResampler\n",
    "from filterflow.base import State\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedPointCloud(tf.Module):\n",
    "    def __init__(self, n_particles, dimension, name=\"OptimizedPointCloud\"):\n",
    "        super(OptimizedPointCloud, self).__init__(name=name)\n",
    "        self._n_particles = n_particles\n",
    "        self._dimension = dimension\n",
    "                \n",
    "        self.cost_and_weight_repr = tf.Variable(tf.random.normal([2,]), name='mix_weights')\n",
    "        self._contribution_weight = tf.Variable(tf.random.normal([self._n_particles, self._n_particles]), name='contribution_weights')\n",
    "    \n",
    "    @tf.function\n",
    "    def _mix_cost_and_weight(self, cost, log_weights):\n",
    "        weighted_cost = self.cost_and_weight_repr[0] * cost\n",
    "        weighted_log_weights = self.cost_and_weight_repr[1] * log_weights\n",
    "        \n",
    "        temp = weighted_cost + tf.expand_dims(weighted_log_weights, 1)\n",
    "        return 2. * tf.math.sigmoid(temp) - 1.\n",
    "    \n",
    "    @tf.function\n",
    "    def _make_contribution(self, transformed_input):\n",
    "        temp = tf.matmul(self._contribution_weight, transformed_input)\n",
    "        return tf.nn.softmax(temp)\n",
    "        \n",
    "    @tf.function\n",
    "    def _input_transform(self, log_w, x):\n",
    "        return self._mix_cost_and_weight(cost(x, x), log_w)\n",
    "        \n",
    "    @tf.function\n",
    "    def _normalize(self, x):\n",
    "        mean = tf.reduce_mean(x, 1, keepdims=True)\n",
    "        x_ = x - mean\n",
    "        std = tf.math.reduce_std(x_, 1, keepdims=True)\n",
    "        return mean, std\n",
    "        \n",
    "    def __call__(self, log_w, x):\n",
    "        mean, std = self._normalize(x)\n",
    "        \n",
    "        transformed_input = self._input_transform(log_w, x - mean)\n",
    "        \n",
    "        contribution_weights = self._make_contribution(transformed_input)\n",
    "        float_n_particles = tf.cast(self._n_particles, float)\n",
    "        z =  float_n_particles * tf.linalg.matmul(contribution_weights, x - mean)\n",
    "        \n",
    "        matrix_for_reg = contribution_weights / float_n_particles\n",
    "        \n",
    "        reg_lines = tf.reduce_mean(tf.abs(tf.reduce_sum(matrix_for_reg, 1) - tf.cast(self._n_particles, float) * tf.math.exp(log_w)))\n",
    "        \n",
    "        return z + mean, reg_lines\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 50\n",
    "N = 25\n",
    "D = 2\n",
    "\n",
    "epsilon = tf.constant(0.25)\n",
    "loss = SinkhornLoss(epsilon, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "regularized_transform = RegularisedTransform(epsilon)\n",
    "optimized_point_cloud = OptimizedPointCloud(N, D)\n",
    "systematic_resampler = SystematicResampler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_fig = bplt.figure(animation_duration=0)\n",
    "scatter_fig.layout.height = '500px'\n",
    "scatter_fig.layout.width = '500px'\n",
    "learnt_scatter = bplt.scatter([], [], size=[], colors = ['blue'])\n",
    "regularized_scatter = bplt.scatter([], [], size=[], colors = ['green'])\n",
    "initial_scatter = bplt.scatter([], [], size=[], colors = ['red'])\n",
    "bplt.set_lim(-2., 2., 'y')\n",
    "_ = bplt.set_lim(-2., 2., 'x')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_na(tensor):\n",
    "    mask = tf.math.is_finite(tensor)\n",
    "    return tf.where(mask, tensor, tf.zeros_like(tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearScale(max=3.0, min=0.0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses_fig = bplt.figure(animation_duration=0)\n",
    "losses_fig.layout.height = '500px'\n",
    "losses_fig.layout.width = '500px'\n",
    "systematic_loss_plot = bplt.plot([], [], colors = ['red'])\n",
    "learnt_loss_plot = bplt.plot([], [], colors = ['blue'])\n",
    "bplt.set_lim(0., 3., 'y')\n",
    "# _ = bplt.set_lim(0., n_iter, 'x')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2dfd9518dee4ed18579d9624a27076f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Figure(axes=[Axis(scale=LinearScale(max=2.0, min=-2.0)), Axis(orientation='vertical', scale=Linâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "HBox([scatter_fig, losses_fig])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 loss: 10.261614799499512, systematic comparison:0.000904199609067291, regularized comparison: -0.0265206266194582, lr: 0.009999999776482582\n",
      "Step 50 loss: 0.11571108549833298, systematic comparison:0.004766217898577452, regularized comparison: -0.030225159600377083, lr: 0.009999999776482582\n",
      "Step 100 loss: 0.027278423309326172, systematic comparison:0.003684626892209053, regularized comparison: -0.02957852929830551, lr: 0.009999999776482582\n",
      "Step 150 loss: 0.030860750004649162, systematic comparison:0.004141117446124554, regularized comparison: -0.02832578495144844, lr: 0.008999999612569809\n",
      "Step 200 loss: 0.014565377496182919, systematic comparison:-0.000919794081710279, regularized comparison: -0.02932380884885788, lr: 0.008099999278783798\n",
      "Step 250 loss: 0.03676600381731987, systematic comparison:0.005799758248031139, regularized comparison: -0.024094391614198685, lr: 0.008099999278783798\n",
      "Step 300 loss: 0.025295810773968697, systematic comparison:9.612459507479798e-06, regularized comparison: -0.025867635384202003, lr: 0.0072899991646409035\n",
      "Step 350 loss: 0.01763097383081913, systematic comparison:0.00693138362839818, regularized comparison: -0.02554917149245739, lr: 0.006560998968780041\n",
      "Step 400 loss: 0.016099965199828148, systematic comparison:0.003992326091974974, regularized comparison: -0.026780445128679276, lr: 0.006560998968780041\n",
      "Step 450 loss: 0.02994590438902378, systematic comparison:0.00619895476847887, regularized comparison: -0.02122342586517334, lr: 0.005904898978769779\n",
      "Step 500 loss: 0.008653971366584301, systematic comparison:0.0023752590641379356, regularized comparison: -0.02913011983036995, lr: 0.005904898978769779\n",
      "Step 550 loss: 0.021421318873763084, systematic comparison:0.001966141164302826, regularized comparison: -0.02588532492518425, lr: 0.005904898978769779\n",
      "Step 600 loss: 0.01855148747563362, systematic comparison:0.004670191556215286, regularized comparison: -0.029137050732970238, lr: 0.005904898978769779\n",
      "Step 650 loss: 0.017192093655467033, systematic comparison:0.002419658936560154, regularized comparison: -0.029896173626184464, lr: 0.005314408801496029\n",
      "Step 700 loss: 0.014946325682103634, systematic comparison:0.0021176692098379135, regularized comparison: -0.02791677415370941, lr: 0.005314408801496029\n",
      "Step 750 loss: 0.01349427830427885, systematic comparison:0.002799311885610223, regularized comparison: -0.02944728545844555, lr: 0.005314408801496029\n",
      "Step 800 loss: 0.021371016278862953, systematic comparison:0.002443981822580099, regularized comparison: -0.02578016370534897, lr: 0.004782967735081911\n",
      "Step 850 loss: 0.018523452803492546, systematic comparison:0.005290687084197998, regularized comparison: -0.029576793313026428, lr: 0.004782967735081911\n",
      "Step 900 loss: 0.01792760007083416, systematic comparison:0.0036631484981626272, regularized comparison: -0.027460748329758644, lr: 0.004304670728743076\n",
      "Step 950 loss: 0.014737904071807861, systematic comparison:0.001329899183474481, regularized comparison: -0.02775428257882595, lr: 0.004304670728743076\n",
      "Step 1000 loss: 0.017621489241719246, systematic comparison:0.004468055441975594, regularized comparison: -0.024341650307178497, lr: 0.004304670728743076\n",
      "Step 1050 loss: 0.014959193766117096, systematic comparison:0.008272461593151093, regularized comparison: -0.026021642610430717, lr: 0.004304670728743076\n",
      "Step 1100 loss: 0.010477295145392418, systematic comparison:0.0034080541227012873, regularized comparison: -0.025008779019117355, lr: 0.004304670728743076\n",
      "Step 1150 loss: 0.020420469343662262, systematic comparison:0.005790475755929947, regularized comparison: -0.02770972065627575, lr: 0.004304670728743076\n",
      "Step 1200 loss: 0.010736411437392235, systematic comparison:0.006547140423208475, regularized comparison: -0.02480081468820572, lr: 0.0038742036558687687\n",
      "Step 1250 loss: 0.015971172600984573, systematic comparison:0.0007192365592345595, regularized comparison: -0.028599049896001816, lr: 0.0038742036558687687\n",
      "Step 1300 loss: 0.02028096653521061, systematic comparison:0.0029181968420743942, regularized comparison: -0.02645955979824066, lr: 0.003486783243715763\n",
      "Step 1350 loss: 0.01252144668251276, systematic comparison:0.009268702007830143, regularized comparison: -0.025074496865272522, lr: 0.003486783243715763\n",
      "Step 1400 loss: 0.012141669169068336, systematic comparison:0.006092566065490246, regularized comparison: -0.028065090999007225, lr: 0.003486783243715763\n",
      "Step 1450 loss: 0.01487825158983469, systematic comparison:0.0014723394997417927, regularized comparison: -0.029002180323004723, lr: 0.003486783243715763\n",
      "Step 1500 loss: 0.015950627624988556, systematic comparison:0.005402249749749899, regularized comparison: -0.025119028985500336, lr: 0.003486783243715763\n",
      "Step 1550 loss: 0.015051157213747501, systematic comparison:0.0034292752388864756, regularized comparison: -0.028693772852420807, lr: 0.003486783243715763\n",
      "Step 1600 loss: 0.015244562178850174, systematic comparison:0.00802750326693058, regularized comparison: -0.03067854605615139, lr: 0.003486783243715763\n",
      "Step 1650 loss: 0.015169684775173664, systematic comparison:0.007253290619701147, regularized comparison: -0.027872305363416672, lr: 0.003486783243715763\n",
      "Step 1700 loss: -0.0044525396078825, systematic comparison:0.00042421233956702054, regularized comparison: -0.03554920107126236, lr: 0.003486783243715763\n",
      "Step 1750 loss: 0.016400467604398727, systematic comparison:0.005019782111048698, regularized comparison: -0.025301262736320496, lr: 0.003486783243715763\n",
      "Step 1800 loss: 0.015415606088936329, systematic comparison:3.098897650488652e-05, regularized comparison: -0.02970556914806366, lr: 0.003486783243715763\n",
      "Step 1850 loss: 0.0016104958485811949, systematic comparison:0.0012348868185654283, regularized comparison: -0.03198237717151642, lr: 0.0031381049193441868\n",
      "Step 1900 loss: 0.007416633423417807, systematic comparison:-0.0010633430210873485, regularized comparison: -0.033502865582704544, lr: 0.0031381049193441868\n",
      "Step 1950 loss: 0.011590305715799332, systematic comparison:0.004239896312355995, regularized comparison: -0.026950353756546974, lr: 0.0031381049193441868\n",
      "Step 2000 loss: 0.012677260674536228, systematic comparison:0.005213339813053608, regularized comparison: -0.024574806913733482, lr: 0.0028242943808436394\n",
      "Step 2050 loss: 0.008898223750293255, systematic comparison:0.00660719582810998, regularized comparison: -0.02924884483218193, lr: 0.0028242943808436394\n",
      "Step 2100 loss: 0.015210851095616817, systematic comparison:0.007966415025293827, regularized comparison: -0.027538036927580833, lr: 0.0028242943808436394\n",
      "Step 2150 loss: 0.013335710391402245, systematic comparison:0.004028229508548975, regularized comparison: -0.02784750610589981, lr: 0.0028242943808436394\n",
      "Step 2200 loss: 0.015099773183465004, systematic comparison:0.0014586285687983036, regularized comparison: -0.02928810752928257, lr: 0.0028242943808436394\n",
      "Step 2250 loss: 0.008235614746809006, systematic comparison:0.010717647150158882, regularized comparison: -0.02861412987112999, lr: 0.002541864989325404\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(666)\n",
    "patience = 25\n",
    "has_waited = 0\n",
    "lr_decay = 0.9\n",
    "current_loss = 1e5\n",
    "n_descent_per_batch = 1\n",
    "\n",
    "lr = tf.Variable(1e-2)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(lr=lr)\n",
    "\n",
    "trainable_variables = optimized_point_cloud.trainable_variables\n",
    "\n",
    "uniform_log_weights = tf.zeros([B, N]) - math.log(N)\n",
    "uniform_weights = tf.zeros([B, N]) + 1/N\n",
    "\n",
    "systematic_losses = []\n",
    "learnt_losses = []\n",
    "\n",
    "for i in range(n_iter):\n",
    "    random_x = tf.random.uniform([B, N, D], -1., 1.)\n",
    "    random_w = tf.random.uniform([B, N], 0., 1.) ** 2\n",
    "    random_w /= tf.reduce_sum(random_w, 1, keepdims=True)\n",
    "    random_log_w = tf.math.log(random_w)\n",
    "    for _ in range(n_descent_per_batch):\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(trainable_variables)\n",
    "            generated_particules, reg = optimized_point_cloud(random_log_w, random_x)\n",
    "            batch_loss = loss(random_log_w, random_w, random_x, uniform_log_weights, uniform_weights, generated_particules)\n",
    "            loss_value = tf.reduce_mean(batch_loss + reg)\n",
    "            gradients = tape.gradient(loss_value, trainable_variables)\n",
    "        optimizer.apply_gradients([(fill_na(grad), var) for grad, var in zip(gradients, trainable_variables)])\n",
    "    \n",
    "        \n",
    "    if loss_value.numpy().sum() < current_loss:\n",
    "        has_waited = 0\n",
    "        current_loss = loss_value.numpy().sum()\n",
    "    else:\n",
    "        has_waited += 1\n",
    "        \n",
    "    if has_waited >= patience:\n",
    "        current_loss = 1e5\n",
    "        lr.assign(lr * lr_decay)\n",
    "        has_waited = 0\n",
    "        patience = min( patience + 10, 200)\n",
    "        \n",
    "    state = State(random_x, random_log_w, random_w, tf.zeros([B,]))\n",
    "    flags = tf.ones([B,], dtype=bool)\n",
    "    systematic_state = systematic_resampler.apply(state, flags)\n",
    "    systematic_loss = tf.reduce_mean(loss(random_log_w, random_w, random_x, uniform_log_weights, uniform_weights, systematic_state.particles))\n",
    "    \n",
    "    systematic_loss_plot.x = np.arange(i)\n",
    "    learnt_loss_plot.x = np.arange(i)\n",
    "    \n",
    "    learnt_losses.append(tf.reduce_mean(batch_loss).numpy().sum())\n",
    "    systematic_losses.append(systematic_loss.numpy().sum())\n",
    "    \n",
    "    \n",
    "    systematic_loss_plot.y = systematic_losses\n",
    "    learnt_loss_plot.y = learnt_losses\n",
    "    \n",
    "    if i % 50 == 0:\n",
    "\n",
    "        regularized_state = regularized_transform.apply(state, flags)\n",
    "        regularized_loss = tf.reduce_mean(loss(random_log_w, random_w, random_x, uniform_log_weights, uniform_weights, regularized_state.particles))\n",
    "\n",
    "        \n",
    "        learnt_scatter.x = generated_particules[0, :, 0]\n",
    "        learnt_scatter.y = generated_particules[0, :, 1]\n",
    "        \n",
    "        initial_scatter.x = random_x[0, :, 0]\n",
    "        initial_scatter.y = random_x[0, :, 1]\n",
    "        initial_scatter.size = random_w[0] * 100\n",
    "        \n",
    "        regularized_scatter.x = regularized_state.particles[0, :, 0]\n",
    "        regularized_scatter.y = regularized_state.particles[0, :, 1]\n",
    "        \n",
    "        print(f'Step {i} loss: {tf.reduce_mean(batch_loss)}, systematic comparison:{systematic_loss}, regularized comparison: {regularized_loss}, lr: {lr.numpy().sum()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_x = tf.random.uniform([B, N, D], -4., 1.)\n",
    "random_w = tf.random.uniform([B, N], 0., 1.) ** 3\n",
    "\n",
    "random_w /= tf.reduce_sum(random_w, 1, keepdims=True)\n",
    "random_log_w = tf.math.log(random_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 / tf.reduce_sum(random_w ** 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_particles = optimized_point_cloud(random_log_w, random_x)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_particles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
