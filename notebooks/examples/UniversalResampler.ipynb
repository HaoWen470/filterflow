{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import bqplot.pyplot as bplt\n",
    "from ipywidgets import HBox\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "from filterflow.resampling.differentiable.loss.regularized import SinkhornLoss\n",
    "from filterflow.resampling.differentiable.loss.regularized import SinkhornLoss\n",
    "from filterflow.resampling.differentiable.regularized_transport.utils import cost\n",
    "from filterflow.resampling.differentiable.biased import RegularisedTransform\n",
    "from filterflow.resampling.standard.systematic import SystematicResampler\n",
    "from filterflow.base import State\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedPointCloud(tf.Module):\n",
    "    def __init__(self, n_particles, dimension, name=\"OptimizedPointCloud\"):\n",
    "        super(OptimizedPointCloud, self).__init__(name=name)\n",
    "        self._n_particles = n_particles\n",
    "        self._dimension = dimension\n",
    "                \n",
    "        self.cost_and_weight_repr = tf.Variable(tf.random.uniform([2,], -0.5, 0.5), name='mix_weights')\n",
    "        self._contribution_weight_1 = tf.Variable(tf.random.uniform([self._n_particles, self._n_particles//2], -1./n_particles, 1./n_particles), name='contribution_weights_1')\n",
    "        self._contribution_weight_2 = tf.Variable(tf.random.uniform([self._n_particles//2, self._n_particles], -1./n_particles, 1./n_particles), name='contribution_weights_2')\n",
    "    \n",
    "    @tf.function\n",
    "    def _mix_cost_and_weight(self, cost, log_weights):\n",
    "        float_n_particles = tf.cast(self._n_particles, float)\n",
    "        weighted_cost = self.cost_and_weight_repr[0] * cost\n",
    "        weighted_log_weights = self.cost_and_weight_repr[1] * (log_weights - tf.math.log(float_n_particles))\n",
    "        \n",
    "        temp = weighted_cost + tf.expand_dims(weighted_log_weights, 1)\n",
    "        return 2. * tf.math.sigmoid(temp) -1.\n",
    "    \n",
    "    @tf.function\n",
    "    def _make_contribution(self, transformed_input):\n",
    "        temp = tf.matmul(transformed_input, self._contribution_weight_1)\n",
    "        temp = 2. * tf.math.sigmoid(temp) - 1.\n",
    "        temp = tf.matmul(temp, self._contribution_weight_2)\n",
    "        return temp\n",
    "        \n",
    "    @tf.function\n",
    "    def _input_transform(self, log_w, x):\n",
    "        return self._mix_cost_and_weight(cost(x, x), log_w)\n",
    "        \n",
    "    @tf.function\n",
    "    def _normalize(self, x):\n",
    "        mean = tf.reduce_mean(x, 1, keepdims=True)\n",
    "        x_ = x - mean\n",
    "        std = tf.math.reduce_std(x_, 1, keepdims=True)\n",
    "        return mean, std\n",
    "        \n",
    "    def __call__(self, log_w, x):\n",
    "        mean, std = self._normalize(x)\n",
    "        \n",
    "        transformed_input = self._input_transform(log_w, (x - mean)/std)\n",
    "\n",
    "        float_n_particles = tf.cast(self._n_particles, float)\n",
    "\n",
    "        log_contribution_weights = self._make_contribution(transformed_input)\n",
    "        log_contribution_weights = log_contribution_weights - tf.reduce_logsumexp(log_contribution_weights, 2, keepdims=True)\n",
    "        log_contribution_weights = tf.math.log(float_n_particles) + log_contribution_weights + tf.expand_dims(log_w, 1)\n",
    "        contribution_weights = tf.exp(log_contribution_weights)\n",
    "#         contribution_weights = float_n_particles * contribution_weights\n",
    "        z =  tf.linalg.matmul(contribution_weights, (x - mean)/std)\n",
    "        \n",
    "        reg_lines = tf.reduce_sum(tf.abs(tf.reduce_sum(contribution_weights, 1) - float_n_particles* tf.math.exp(log_w)))\n",
    "        reg_cols = tf.reduce_sum(tf.abs(tf.reduce_sum(contribution_weights, 2) - 1.))\n",
    "        return std*(z + mean), reg_lines + reg_cols\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 10\n",
    "N = 25\n",
    "D = 2\n",
    "\n",
    "epsilon = tf.constant(0.25)\n",
    "loss = SinkhornLoss(epsilon, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "regularized_transform = RegularisedTransform(epsilon)\n",
    "optimized_point_cloud = OptimizedPointCloud(N, D)\n",
    "systematic_resampler = SystematicResampler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_fig = bplt.figure(animation_duration=0)\n",
    "scatter_fig.layout.height = '500px'\n",
    "scatter_fig.layout.width = '500px'\n",
    "learnt_scatter = bplt.scatter([], [], size=[], colors = ['blue'])\n",
    "regularized_scatter = bplt.scatter([], [], size=[], colors = ['green'], alpha=0.75)\n",
    "initial_scatter = bplt.scatter([], [], size=[], colors = ['red'])\n",
    "bplt.set_lim(-5., 5., 'y')\n",
    "_ = bplt.set_lim(-5., 5., 'x')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_na(tensor):\n",
    "    mask = tf.math.is_finite(tensor)\n",
    "    return tf.where(mask, tensor, tf.zeros_like(tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearScale(max=3.0, min=0.0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses_fig = bplt.figure(animation_duration=0)\n",
    "losses_fig.layout.height = '500px'\n",
    "losses_fig.layout.width = '500px'\n",
    "systematic_loss_plot = bplt.plot([], [], colors = ['red'])\n",
    "learnt_loss_plot = bplt.plot([], [], colors = ['blue'])\n",
    "bplt.set_lim(0., 3., 'y')\n",
    "# _ = bplt.set_lim(0., n_iter, 'x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6502ac2c7bf4ac08e0b25d8a4d45667",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Figure(axes=[Axis(scale=LinearScale(max=5.0, min=-5.0)), Axis(orientation='vertical', scale=Linâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "HBox([scatter_fig, losses_fig])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 loss: 0.7448053956031799, systematic comparison:0.03441043943166733, regularized comparison: 0.029747450724244118, lr: 0.009999999776482582, reg: 0.3794863820075989\n",
      "Step 50 loss: 0.8223794102668762, systematic comparison:0.03613913804292679, regularized comparison: 0.0281593706458807, lr: 0.009999999776482582, reg: 0.019738195464015007\n",
      "Step 100 loss: 0.7418352365493774, systematic comparison:0.0219185259193182, regularized comparison: 0.028830811381340027, lr: 0.008999999612569809, reg: 0.0032449164427816868\n",
      "Step 150 loss: 0.7918053865432739, systematic comparison:0.03258129954338074, regularized comparison: 0.06700434535741806, lr: 0.008099999278783798, reg: 0.0007443295326083899\n",
      "Step 200 loss: 0.7546194791793823, systematic comparison:0.03899209946393967, regularized comparison: 0.02625749073922634, lr: 0.008099999278783798, reg: 0.0003857287229038775\n",
      "Step 250 loss: 0.9041797518730164, systematic comparison:0.0411408506333828, regularized comparison: 0.0335797555744648, lr: 0.0072899991646409035, reg: 0.00019841035827994347\n",
      "Step 300 loss: 0.7320217490196228, systematic comparison:0.03798236697912216, regularized comparison: 0.028047379106283188, lr: 0.0072899991646409035, reg: 0.0001130634336732328\n",
      "Step 350 loss: 0.759304940700531, systematic comparison:0.035660676658153534, regularized comparison: 0.0338028185069561, lr: 0.0072899991646409035, reg: 8.085934678092599e-05\n",
      "Step 400 loss: 0.6711243987083435, systematic comparison:0.029811138287186623, regularized comparison: 0.026646777987480164, lr: 0.006560998968780041, reg: 6.713173206662759e-05\n",
      "Step 450 loss: 0.7620696425437927, systematic comparison:0.043250393122434616, regularized comparison: 0.032465677708387375, lr: 0.006560998968780041, reg: 6.763870624126866e-05\n",
      "Step 500 loss: 0.7454183101654053, systematic comparison:0.0362086221575737, regularized comparison: 0.022341815754771233, lr: 0.006560998968780041, reg: 6.860287976451218e-05\n",
      "Step 550 loss: 0.7092353105545044, systematic comparison:0.02788236178457737, regularized comparison: 0.032488755881786346, lr: 0.005904898978769779, reg: 6.237367051653564e-05\n",
      "Step 600 loss: 0.7036125063896179, systematic comparison:0.027934998273849487, regularized comparison: 0.030740370973944664, lr: 0.005904898978769779, reg: 7.31991240172647e-05\n",
      "Step 650 loss: 0.8591352701187134, systematic comparison:0.03653962165117264, regularized comparison: 0.027847006916999817, lr: 0.005314408801496029, reg: 6.282748654484749e-05\n",
      "Step 700 loss: 0.8422976732254028, systematic comparison:0.05761619284749031, regularized comparison: 0.02382165938615799, lr: 0.005314408801496029, reg: 6.75361297908239e-05\n",
      "Step 750 loss: 0.8032673001289368, systematic comparison:0.01810419373214245, regularized comparison: 0.027639437466859818, lr: 0.005314408801496029, reg: 7.173407357186079e-05\n",
      "Step 800 loss: 1.0109351873397827, systematic comparison:0.052143968641757965, regularized comparison: 0.0373721607029438, lr: 0.005314408801496029, reg: 6.579553883057088e-05\n",
      "Step 850 loss: 0.7616332173347473, systematic comparison:0.034144673496484756, regularized comparison: 0.024989662691950798, lr: 0.004782967735081911, reg: 7.041597564239055e-05\n",
      "Step 900 loss: 0.8507522344589233, systematic comparison:0.03565605729818344, regularized comparison: 0.03040759265422821, lr: 0.004782967735081911, reg: 6.974464486120269e-05\n",
      "Step 950 loss: 0.6411604881286621, systematic comparison:0.023306366056203842, regularized comparison: 0.027940724045038223, lr: 0.004782967735081911, reg: 7.171845936682075e-05\n",
      "Step 1000 loss: 0.7744843363761902, systematic comparison:0.03228913992643356, regularized comparison: 0.028583761304616928, lr: 0.004782967735081911, reg: 6.819502596044913e-05\n",
      "Step 1050 loss: 0.6829320192337036, systematic comparison:0.03045887127518654, regularized comparison: 0.02811158262193203, lr: 0.004304670728743076, reg: 6.816578388679773e-05\n",
      "Step 1100 loss: 0.9384849667549133, systematic comparison:0.0479302778840065, regularized comparison: 0.030546197667717934, lr: 0.004304670728743076, reg: 7.595556235173717e-05\n",
      "Step 1150 loss: 0.8951103091239929, systematic comparison:0.04189926013350487, regularized comparison: 0.03194880113005638, lr: 0.0038742036558687687, reg: 7.003836799412966e-05\n",
      "Step 1200 loss: 0.8137667775154114, systematic comparison:0.02534489706158638, regularized comparison: 0.03784438222646713, lr: 0.0038742036558687687, reg: 7.122670649550855e-05\n",
      "Step 1250 loss: 0.7812389135360718, systematic comparison:0.03920166939496994, regularized comparison: 0.035567887127399445, lr: 0.003486783243715763, reg: 7.068854756653309e-05\n",
      "Step 1300 loss: 0.7498452663421631, systematic comparison:0.023963365703821182, regularized comparison: 0.027996648102998734, lr: 0.003486783243715763, reg: 6.738708179909736e-05\n",
      "Step 1350 loss: 0.7710233926773071, systematic comparison:0.03354611247777939, regularized comparison: 0.03266475349664688, lr: 0.003486783243715763, reg: 6.08065165579319e-05\n",
      "Step 1400 loss: 0.8641586303710938, systematic comparison:0.04460121691226959, regularized comparison: 0.05153827741742134, lr: 0.0031381049193441868, reg: 7.377032306976616e-05\n",
      "Step 1450 loss: 0.6487375497817993, systematic comparison:0.024862343445420265, regularized comparison: 0.027463268488645554, lr: 0.0031381049193441868, reg: 6.095526987337507e-05\n",
      "Step 1500 loss: 0.8379614949226379, systematic comparison:0.03745148330926895, regularized comparison: 0.04203266277909279, lr: 0.0031381049193441868, reg: 7.520316285081208e-05\n",
      "Step 1550 loss: 0.696277379989624, systematic comparison:0.040550876408815384, regularized comparison: 0.030018433928489685, lr: 0.0031381049193441868, reg: 7.140796515159309e-05\n",
      "Step 1600 loss: 0.7739843130111694, systematic comparison:0.036117345094680786, regularized comparison: 0.032108940184116364, lr: 0.0031381049193441868, reg: 6.124158971942961e-05\n",
      "Step 1650 loss: 0.677676796913147, systematic comparison:0.03180108219385147, regularized comparison: 0.026213264092803, lr: 0.0031381049193441868, reg: 7.445689698215574e-05\n",
      "Step 1700 loss: 0.6258881092071533, systematic comparison:0.029519926756620407, regularized comparison: 0.02728705108165741, lr: 0.0028242943808436394, reg: 6.478513387264684e-05\n",
      "Step 1750 loss: 0.8074396252632141, systematic comparison:0.05614262819290161, regularized comparison: 0.03340626507997513, lr: 0.0028242943808436394, reg: 6.038538413122296e-05\n",
      "Step 1800 loss: 0.5939390063285828, systematic comparison:0.02633160911500454, regularized comparison: 0.02302524447441101, lr: 0.0028242943808436394, reg: 7.000026380410418e-05\n",
      "Step 1850 loss: 0.9085226058959961, systematic comparison:0.04396010935306549, regularized comparison: 0.04530836641788483, lr: 0.0028242943808436394, reg: 6.657664926024154e-05\n",
      "Step 1900 loss: 0.8658806085586548, systematic comparison:0.02706103026866913, regularized comparison: 0.035306788980960846, lr: 0.0028242943808436394, reg: 6.835422391304746e-05\n",
      "Step 1950 loss: 0.7700415849685669, systematic comparison:0.025465335696935654, regularized comparison: 0.025912228971719742, lr: 0.0028242943808436394, reg: 6.783584831282496e-05\n",
      "Step 2000 loss: 0.8334106206893921, systematic comparison:0.03683408722281456, regularized comparison: 0.028120998293161392, lr: 0.0028242943808436394, reg: 7.259083940880373e-05\n",
      "Step 2050 loss: 0.6738635301589966, systematic comparison:0.04114257171750069, regularized comparison: 0.03617306426167488, lr: 0.0028242943808436394, reg: 6.380717968568206e-05\n",
      "Step 2100 loss: 0.8333209156990051, systematic comparison:0.048079051077365875, regularized comparison: 0.03982704505324364, lr: 0.002541864989325404, reg: 7.202354026958346e-05\n",
      "Step 2150 loss: 0.6850420236587524, systematic comparison:0.03598904237151146, regularized comparison: 0.02584322728216648, lr: 0.002541864989325404, reg: 6.976729491725564e-05\n",
      "Step 2200 loss: 0.749761700630188, systematic comparison:0.027092207223176956, regularized comparison: 0.028497958555817604, lr: 0.002541864989325404, reg: 6.142464553704485e-05\n",
      "Step 2250 loss: 0.7848192453384399, systematic comparison:0.04858597740530968, regularized comparison: 0.027753179892897606, lr: 0.002541864989325404, reg: 5.889794556424022e-05\n",
      "Step 2300 loss: 0.7771914005279541, systematic comparison:0.04187634587287903, regularized comparison: 0.03238809481263161, lr: 0.002541864989325404, reg: 6.920143641764298e-05\n",
      "Step 2350 loss: 0.8172271847724915, systematic comparison:0.04123597964644432, regularized comparison: 0.034636545926332474, lr: 0.002541864989325404, reg: 6.977670273045078e-05\n",
      "Step 2400 loss: 0.6440302729606628, systematic comparison:0.02932451106607914, regularized comparison: 0.033687904477119446, lr: 0.002541864989325404, reg: 7.357276626862586e-05\n",
      "Step 2450 loss: 0.8040496706962585, systematic comparison:0.027986004948616028, regularized comparison: 0.02567833662033081, lr: 0.0022876784205436707, reg: 5.993056402076036e-05\n",
      "Step 2500 loss: 0.8669145703315735, systematic comparison:0.03209057077765465, regularized comparison: 0.029830392450094223, lr: 0.0022876784205436707, reg: 6.248419231269509e-05\n",
      "Step 2550 loss: 0.7290843725204468, systematic comparison:0.039289478212594986, regularized comparison: 0.03204701840877533, lr: 0.0022876784205436707, reg: 7.089957944117486e-05\n",
      "Step 2600 loss: 0.8547992706298828, systematic comparison:0.05112544447183609, regularized comparison: 0.03738347440958023, lr: 0.0022876784205436707, reg: 6.70582230668515e-05\n",
      "Step 2650 loss: 0.6360740065574646, systematic comparison:0.03158318251371384, regularized comparison: 0.032375268638134, lr: 0.0022876784205436707, reg: 6.411685171769932e-05\n",
      "Step 2700 loss: 0.7912757992744446, systematic comparison:0.040510084480047226, regularized comparison: 0.02714931033551693, lr: 0.0020589104387909174, reg: 6.486743222922087e-05\n",
      "Step 2750 loss: 0.9421225786209106, systematic comparison:0.03773416206240654, regularized comparison: 0.035583093762397766, lr: 0.0020589104387909174, reg: 7.466231909347698e-05\n",
      "Step 2800 loss: 1.0052257776260376, systematic comparison:0.03566142916679382, regularized comparison: 0.04083947092294693, lr: 0.0020589104387909174, reg: 6.307903095148504e-05\n",
      "Step 2850 loss: 0.6929885149002075, systematic comparison:0.03351170942187309, regularized comparison: 0.02504144050180912, lr: 0.0020589104387909174, reg: 7.480298518203199e-05\n",
      "Step 2900 loss: 0.7299246788024902, systematic comparison:0.03377415984869003, regularized comparison: 0.03037717007100582, lr: 0.001853019348345697, reg: 7.03461846569553e-05\n",
      "Step 2950 loss: 0.7028127312660217, systematic comparison:0.03639168292284012, regularized comparison: 0.026626959443092346, lr: 0.001853019348345697, reg: 6.459926953539252e-05\n",
      "Step 3000 loss: 0.7459908723831177, systematic comparison:0.03692210838198662, regularized comparison: 0.022387513890862465, lr: 0.001853019348345697, reg: 7.618538802489638e-05\n",
      "Step 3050 loss: 0.8495301008224487, systematic comparison:0.04779018834233284, regularized comparison: 0.031722165644168854, lr: 0.001853019348345697, reg: 6.233446765691042e-05\n",
      "Step 3100 loss: 0.773474395275116, systematic comparison:0.03994692489504814, regularized comparison: 0.029311398044228554, lr: 0.001853019348345697, reg: 6.455967377405614e-05\n",
      "Step 3150 loss: 0.7918682098388672, systematic comparison:0.026912812143564224, regularized comparison: 0.028770804405212402, lr: 0.001853019348345697, reg: 6.615192978642881e-05\n",
      "Step 3200 loss: 0.8090013265609741, systematic comparison:0.041576460003852844, regularized comparison: 0.027065131813287735, lr: 0.0016677173553034663, reg: 7.576151983812451e-05\n",
      "Step 3250 loss: 0.9159566760063171, systematic comparison:0.030234932899475098, regularized comparison: 0.038341835141181946, lr: 0.0016677173553034663, reg: 6.992902490310371e-05\n",
      "Step 3300 loss: 0.7980314493179321, systematic comparison:0.036884479224681854, regularized comparison: 0.03425367921590805, lr: 0.0016677173553034663, reg: 7.25843055988662e-05\n",
      "Step 3350 loss: 0.7995302677154541, systematic comparison:0.04917991906404495, regularized comparison: 0.026985427364706993, lr: 0.0016677173553034663, reg: 6.619816122110933e-05\n",
      "Step 3400 loss: 0.693496584892273, systematic comparison:0.042856961488723755, regularized comparison: 0.026387516409158707, lr: 0.0016677173553034663, reg: 5.9182289987802505e-05\n",
      "Step 3450 loss: 0.8184356689453125, systematic comparison:0.03737584501504898, regularized comparison: 0.03535793721675873, lr: 0.0016677173553034663, reg: 7.90866615716368e-05\n",
      "Step 3500 loss: 0.7338651418685913, systematic comparison:0.030184801667928696, regularized comparison: 0.03248106688261032, lr: 0.0016677173553034663, reg: 6.865979230497032e-05\n",
      "Step 3550 loss: 0.7402783632278442, systematic comparison:0.026709455996751785, regularized comparison: 0.026872653514146805, lr: 0.0016677173553034663, reg: 6.931573443580419e-05\n",
      "Step 3600 loss: 0.8044813871383667, systematic comparison:0.03892876207828522, regularized comparison: 0.02923983335494995, lr: 0.0015009455382823944, reg: 6.651779403910041e-05\n",
      "Step 3650 loss: 0.7991777062416077, systematic comparison:0.046220991760492325, regularized comparison: 0.03859260305762291, lr: 0.0015009455382823944, reg: 6.73691974952817e-05\n",
      "Step 3700 loss: 0.6785644292831421, systematic comparison:0.03461166471242905, regularized comparison: 0.025662248954176903, lr: 0.0015009455382823944, reg: 7.01346289133653e-05\n",
      "Step 3750 loss: 0.7375656962394714, systematic comparison:0.04244663566350937, regularized comparison: 0.028239313513040543, lr: 0.0015009455382823944, reg: 6.8583911343012e-05\n",
      "Step 3800 loss: 0.7208529710769653, systematic comparison:0.038948822766542435, regularized comparison: 0.03150777891278267, lr: 0.0015009455382823944, reg: 7.183098932728171e-05\n",
      "Step 3850 loss: 0.7340997457504272, systematic comparison:0.027247106656432152, regularized comparison: 0.027501296252012253, lr: 0.0015009455382823944, reg: 6.060209852876142e-05\n",
      "Step 3900 loss: 0.894914448261261, systematic comparison:0.03460099175572395, regularized comparison: 0.030337965115904808, lr: 0.0015009455382823944, reg: 6.33427916909568e-05\n",
      "Step 3950 loss: 0.6447323560714722, systematic comparison:0.03285451978445053, regularized comparison: 0.02937513217329979, lr: 0.0015009455382823944, reg: 6.833114457549527e-05\n",
      "Step 4000 loss: 0.7655852437019348, systematic comparison:0.034963853657245636, regularized comparison: 0.03764187544584274, lr: 0.0013508509146049619, reg: 6.812002538936213e-05\n",
      "Step 4050 loss: 0.9151994585990906, systematic comparison:0.02866443060338497, regularized comparison: 0.027088234201073647, lr: 0.0013508509146049619, reg: 5.694109131582081e-05\n",
      "Step 4100 loss: 0.6911398768424988, systematic comparison:0.04536538943648338, regularized comparison: 0.025058794766664505, lr: 0.0013508509146049619, reg: 5.795710603706539e-05\n",
      "Step 4150 loss: 0.7731221914291382, systematic comparison:0.040463779121637344, regularized comparison: 0.027818743139505386, lr: 0.0013508509146049619, reg: 8.097235695458949e-05\n",
      "Step 4200 loss: 0.7612754106521606, systematic comparison:0.04767397791147232, regularized comparison: 0.037179164588451385, lr: 0.0013508509146049619, reg: 6.525940261781216e-05\n",
      "Step 4250 loss: 0.5386146306991577, systematic comparison:0.028129369020462036, regularized comparison: 0.025511866435408592, lr: 0.0013508509146049619, reg: 7.015644951025024e-05\n",
      "Step 4300 loss: 0.9521896243095398, systematic comparison:0.029520440846681595, regularized comparison: 0.028403859585523605, lr: 0.0013508509146049619, reg: 6.726691935909912e-05\n",
      "Step 4350 loss: 0.763648271560669, systematic comparison:0.02887975238263607, regularized comparison: 0.0311894528567791, lr: 0.0013508509146049619, reg: 7.58581591071561e-05\n",
      "Step 4400 loss: 0.685043215751648, systematic comparison:0.029668670147657394, regularized comparison: 0.025988835841417313, lr: 0.0013508509146049619, reg: 6.302411202341318e-05\n",
      "Step 4450 loss: 0.8213788270950317, systematic comparison:0.03600360080599785, regularized comparison: 0.02662784792482853, lr: 0.0012157658347859979, reg: 7.406092481687665e-05\n",
      "Step 4500 loss: 0.7150920629501343, systematic comparison:0.02005661465227604, regularized comparison: 0.028184831142425537, lr: 0.0012157658347859979, reg: 6.448627391364425e-05\n",
      "Step 4550 loss: 0.6642608046531677, systematic comparison:0.0249059796333313, regularized comparison: 0.034778401255607605, lr: 0.0012157658347859979, reg: 6.806448800489306e-05\n",
      "Step 4600 loss: 0.6919527053833008, systematic comparison:0.04098253697156906, regularized comparison: 0.030343782156705856, lr: 0.0012157658347859979, reg: 6.564029899891466e-05\n",
      "Step 4650 loss: 0.8035653829574585, systematic comparison:0.03246342018246651, regularized comparison: 0.03509024903178215, lr: 0.0012157658347859979, reg: 6.079423474147916e-05\n",
      "Step 4700 loss: 0.7760533690452576, systematic comparison:0.042197369039058685, regularized comparison: 0.024933988228440285, lr: 0.001094189239665866, reg: 6.989362009335309e-05\n",
      "Step 4750 loss: 0.6973534822463989, systematic comparison:0.030281567946076393, regularized comparison: 0.026739854365587234, lr: 0.001094189239665866, reg: 6.325799768092111e-05\n",
      "Step 4800 loss: 0.7025343775749207, systematic comparison:0.025307735428214073, regularized comparison: 0.02312396839261055, lr: 0.001094189239665866, reg: 7.068458944559097e-05\n",
      "Step 4850 loss: 0.7100610136985779, systematic comparison:0.025766586884856224, regularized comparison: 0.028473585844039917, lr: 0.001094189239665866, reg: 6.76183044561185e-05\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(666)\n",
    "patience = 25\n",
    "has_waited = 0\n",
    "lr_decay = 0.9\n",
    "current_loss = 1e5\n",
    "n_descent_per_batch = 1\n",
    "\n",
    "lr = tf.Variable(1e-2)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(lr=lr)\n",
    "\n",
    "trainable_variables = optimized_point_cloud.trainable_variables\n",
    "\n",
    "uniform_log_weights = tf.zeros([B, N]) - math.log(N)\n",
    "uniform_weights = tf.zeros([B, N]) + 1/N\n",
    "\n",
    "systematic_losses = []\n",
    "learnt_losses = []\n",
    "\n",
    "for i in range(n_iter):\n",
    "    random_x = tf.random.normal([B, N, D], -1., 1.)\n",
    "    random_w = tf.random.uniform([B, N], 0., 1.) ** 2\n",
    "    random_w /= tf.reduce_sum(random_w, 1, keepdims=True)\n",
    "    random_log_w = tf.math.log(random_w)\n",
    "    for _ in range(n_descent_per_batch):\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(trainable_variables)\n",
    "            generated_particules, reg = optimized_point_cloud(random_log_w, random_x)\n",
    "            batch_loss = loss(random_log_w, random_w, random_x, uniform_log_weights, uniform_weights, generated_particules)\n",
    "            loss_value = tf.reduce_mean(batch_loss + reg)\n",
    "            gradients = tape.gradient(loss_value, trainable_variables)\n",
    "        optimizer.apply_gradients([(fill_na(grad), var) for grad, var in zip(gradients, trainable_variables)])\n",
    "    \n",
    "        \n",
    "    if loss_value.numpy().sum() < current_loss:\n",
    "        has_waited = 0\n",
    "        current_loss = loss_value.numpy().sum()\n",
    "    else:\n",
    "        has_waited += 1\n",
    "        \n",
    "    if has_waited >= patience:\n",
    "        current_loss = 1e5\n",
    "        lr.assign(lr * lr_decay)\n",
    "        has_waited = 0\n",
    "        patience = min( patience + 10, 200)\n",
    "        \n",
    "    state = State(random_x, random_log_w, random_w, tf.zeros([B,]))\n",
    "    flags = tf.ones([B,], dtype=bool)\n",
    "    systematic_state = systematic_resampler.apply(state, flags)\n",
    "    systematic_loss = tf.reduce_mean(loss(random_log_w, random_w, random_x, uniform_log_weights, uniform_weights, systematic_state.particles))\n",
    "    \n",
    "    systematic_loss_plot.x = np.arange(i)\n",
    "    learnt_loss_plot.x = np.arange(i)\n",
    "    \n",
    "    learnt_losses.append(tf.reduce_mean(batch_loss).numpy().sum())\n",
    "    systematic_losses.append(systematic_loss.numpy().sum())\n",
    "    \n",
    "    \n",
    "    systematic_loss_plot.y = systematic_losses\n",
    "    learnt_loss_plot.y = learnt_losses\n",
    "    \n",
    "    if i % 50 == 0:\n",
    "\n",
    "        regularized_state = regularized_transform.apply(state, flags)\n",
    "        regularized_loss = tf.reduce_mean(loss(random_log_w, random_w, random_x, uniform_log_weights, uniform_weights, regularized_state.particles))\n",
    "\n",
    "        \n",
    "        learnt_scatter.x = generated_particules[0, :, 0]\n",
    "        learnt_scatter.y = generated_particules[0, :, 1]\n",
    "        \n",
    "        initial_scatter.x = random_x[0, :, 0]\n",
    "        initial_scatter.y = random_x[0, :, 1]\n",
    "        initial_scatter.size = random_w[0] * 100\n",
    "        \n",
    "        regularized_scatter.x = regularized_state.particles[0, :, 0]\n",
    "        regularized_scatter.y = regularized_state.particles[0, :, 1]\n",
    "        \n",
    "        print(f'Step {i} loss: {tf.reduce_mean(batch_loss)}, systematic comparison:{systematic_loss}, regularized comparison: {regularized_loss}, lr: {lr.numpy().sum()}, reg: {reg.numpy().sum()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
